{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the Reliability of MIDI-Derived Annotations\n",
    "\n",
    "Given that a MIDI file can contain a tantalizing array of information about a piece of music, the question remains as to how reliable this information is in MIDI files found \"in the wild\".  To attempt to measure this, we can compare MIDI-derived annotations to human-made annotations.  In the following two experiments, we compare key labels and beat locations extracted from MIDI transcriptions of Beatles songs to annotations from the [Isophonics Beatles dataset](http://isophonics.net/content/reference-annotations-beatles).  Key and beat are the two types of annotations which are available both from MIDI files and Isophonics; conveniently, they also represent alignment-independent timing-agnostic information (key) and alignment-dependent, timing-critical information (beat).  As a short summary of our results, we propose that MIDI-derived annotations are useful, but their usage may come with a variety of task-dependent caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "import glob\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import librosa\n",
    "import vamp\n",
    "import tabulate\n",
    "import djitw\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key evaluation\n",
    "\n",
    "MIDI files can optionally include key change meta-events.  We found 223 Beatles MIDI files which had a single key annotation and compared them to the Isophonics Beatles annotations using the [MIREX methodology](http://www.music-ir.org/mirex/wiki/2015:Audio_Key_Detection) to compute an accuracy score.  For baselines, we also evaluated handmade annotations from [whatkeyisitin.com](http://whatkeyisitin.com) and the [QM-vamp key estimator](http://vamp-plugins.org/plugin-doc/qm-vamp-plugins.html#qm-keydetector), a high-quality audio content-based key estimation algorithm.  If the MIDI-derived annotations are to be considered reliable, they should achieve accuracy scores on par with the handmade annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Key loading/computation functions\n",
    "def load_midi_key(filename):\n",
    "    ''' Load in key labels from a MIDI file '''\n",
    "    # Load in MIDI object and grab key change events\n",
    "    pm = pretty_midi.PrettyMIDI(filename)\n",
    "    key_changes = pm.key_signature_changes\n",
    "    # Convert each key change's number to a string (like 'C Major')\n",
    "    # Also convert it to lowercase, for mir_eval's sake\n",
    "    return [pretty_midi.key_number_to_key_name(k.key_number).lower()\n",
    "            for k in key_changes]\n",
    "\n",
    "def load_isophonics_key(filename):\n",
    "    ''' Read in key labels from an isophonics lab file '''\n",
    "    # Isophonics key lab files have three columns:\n",
    "    # start time, end time, and label\n",
    "    start, end, labels = mir_eval.io.load_delimited(\n",
    "        filename, [float, float, str])\n",
    "    # Extract key labels, which in lab files are formatted as\n",
    "    # 'key\\tC' or 'key\\tC:minor'\n",
    "    keys = [l.split('\\t')[1] for l in labels if 'Key' in l]\n",
    "    # Convert from 'C' and 'C:minor' to 'c major' and 'c minor'\n",
    "    for n, key in enumerate(keys):\n",
    "        if 'minor' in key:\n",
    "            keys[n] = key.replace(':', ' ').lower()\n",
    "        else:\n",
    "            keys[n] = key.lower() + ' major'\n",
    "        # Validate the key early\n",
    "        mir_eval.key.validate_key(keys[n])\n",
    "    return keys\n",
    "\n",
    "def load_vamp_key(filename):\n",
    "    ''' Estimate the key from an audio file using QM key detector '''\n",
    "    # Load in audio data at its native sampling rate\n",
    "    audio_data, fs = librosa.load(filename, sr=None)\n",
    "    # Create a vamp processor that will generate key labels\n",
    "    key_generator = vamp.process_audio_multiple_outputs(\n",
    "        audio_data, fs, 'qm-vamp-plugins:qm-keydetector', ['key'])\n",
    "    # Grab the key labels produced by the vampplugin\n",
    "    vamp_output = [out['key'] for out in key_generator]\n",
    "    keys = [l['label'] for l in vamp_output]\n",
    "    # Compute the durations of each key in the song\n",
    "    starts = [float(l['timestamp']) for l in vamp_output]\n",
    "    starts.append(librosa.get_duration(audio_data, fs))\n",
    "    durations = np.diff(starts)\n",
    "    unique_keys = list(set(keys))\n",
    "    key_durations = [sum(d for k, d in zip(keys, durations) if k == key)\n",
    "                     for key in unique_keys]\n",
    "    # Retrieve the key which spanned the most of the song\n",
    "    most_common_key = unique_keys[np.argmax(key_durations)]\n",
    "    # Sometimes vamp produces keys like\n",
    "    # 'Eb / D# minor'\n",
    "    # so here, we are just retrieving the last part ('D# minor')\n",
    "    if ' / ' in most_common_key:\n",
    "        most_common_key = most_common_key.split(' / ')[1]\n",
    "    return most_common_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file for each source\n",
    "scores = collections.defaultdict(list)\n",
    "# Keep track of whether each MIDI estimated key is C major\n",
    "c_majors = []\n",
    "for lab_filename in glob.glob('data/isophonics_key/*/*.lab'):\n",
    "    # Load Isophonics key from .lab file\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(lab_filename)\n",
    "    except Exception as e:\n",
    "        # Keep track of how many isophonics files which have invalid keys\n",
    "        n_skipped['Isophonics key was invalid'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['Isophonics returned more than 1 key'] += 1\n",
    "        continue\n",
    "    isophonics_key = isophonics_keys[0]\n",
    "    \n",
    "    # Loop over all possible MIDI files for this key\n",
    "    midi_glob = lab_filename.replace('isophonics_key', 'mid').replace('.lab', '.mid*')\n",
    "    for midi_filename in glob.glob(midi_glob):\n",
    "        # Get keys from MIDI file\n",
    "        try:\n",
    "            midi_keys = load_midi_key(midi_filename)\n",
    "        except Exception as e:\n",
    "            n_skipped['MIDI parsing raised an exception'] += 1\n",
    "            continue\n",
    "        # If there's no key change event, skip\n",
    "        if len(midi_keys) == 0:\n",
    "            n_skipped['MIDI file had no key annotations'] += 1\n",
    "            continue\n",
    "        # If there's multiple key change events, skip\n",
    "        if len(midi_keys) > 1:\n",
    "            n_skipped['MIDI file had multiple key annotations'] += len(midi_keys) > 1\n",
    "            continue\n",
    "        midi_key = midi_keys[0]\n",
    "        # Keep track of whether the estimated key was a C major\n",
    "        c_majors.append(midi_keys[0] == 'c major')\n",
    "        # Compute and store score for this MIDI file\n",
    "        scores['midi'].append(mir_eval.key.weighted_score(isophonics_key, midi_key))\n",
    "\n",
    "    # Construct .wav filename from .lab filename\n",
    "    audio_filename = lab_filename.replace('isophonics_key', 'wav').replace('.lab', '.wav')\n",
    "    # Estimate the key using vamp QM key detector plugin\n",
    "    try:\n",
    "        vamp_key = load_vamp_key(audio_filename)\n",
    "    except Exception as e:\n",
    "        n_skipped['reading the audio file raised an exception'] += 1\n",
    "        continue\n",
    "    scores['vamp'].append(mir_eval.key.weighted_score(isophonics_key, vamp_key))\n",
    "\n",
    "    # Construct whatkeyisitin text filename from .lab filename\n",
    "    whatkeyisitin_filename = lab_filename.replace('isophonics_key', 'whatkeyisitin_key').replace('.lab', '.txt')\n",
    "    if not os.path.exists(whatkeyisitin_filename):\n",
    "        # Keep track of how many are skipped due to missing wkiii annotation\n",
    "        n_skipped['whatkeyisitin.com did not have an annotation'] += 1\n",
    "        continue\n",
    "    with open(whatkeyisitin_filename) as f:\n",
    "        whatkeyisitin_key = f.read()\n",
    "    scores['wkiii'].append(mir_eval.key.weighted_score(isophonics_key, whatkeyisitin_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the number of files which were skipped for different reasons\n",
    "for key, value in n_skipped.items():\n",
    "    print '{} skipped because {}'.format(value, key)\n",
    "print 'Total isophonics .lab files: {}'.format(len(glob.glob('data/isophonics_key/*/*.lab')))\n",
    "print\n",
    "# Pretty-print a table of results\n",
    "mean_scores = collections.OrderedDict([\n",
    "    ('MIDI, all keys', np.mean(scores['midi'])),\n",
    "    ('MIDI, C major only', np.mean([s for c, s in zip(c_majors, scores['midi']) if c])),\n",
    "    ('MIDI, non-C major', np.mean([s for c, s in zip(c_majors, scores['midi']) if not c])),\n",
    "    ('QM Vamp Key Detector', np.mean(scores['vamp'])),\n",
    "    ('whatkeyisitin.com', np.mean(scores['wkiii']))])\n",
    "n_comparisons = collections.OrderedDict([\n",
    "    ('MIDI, all keys', len(scores['midi'])),\n",
    "    ('MIDI, C major only', sum(c_majors)),\n",
    "    ('MIDI, non-C major', len([c for c in c_majors if not c])),\n",
    "    ('QM Vamp Key Detector', len(scores['vamp'])),\n",
    "    ('whatkeyisitin.com', len(scores['wkiii']))])\n",
    "print tabulate.tabulate(\n",
    "    [(name, score, num) for (name, score, num) in \n",
    "     zip(mean_scores.keys(), mean_scores.values(), n_comparisons.values())],\n",
    "    ['Source', 'Mean score', '# of comparisons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that in general MIDI-derived key annotations are not very reliable (they achieved a score of 0.4, compared to the audio content-based key estimator's ~0.69 or the human-to-human baseline of ~0.86).  However, a suspicious characteristic of MIDI files is the preponderance of C major key annotations - over 65% of the key annotations in our Beatles MIDI files were C major.  If we evaluate non-C major key annotations separately, we achieve a score of 0.84 - very close to the human-to-human baseline.  This suggests that **non-C major annotations are reliable, but C major annotations are not**.  This may be caused by the fact that some MIDI transcription software packages insert C major key change events by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beat evaluation\n",
    "\n",
    "Because MIDI files are transcriptions, they contain meter information, so it is possible to extract beat (and downbeat) locations from them.  In order to use these MIDI-derived beats as annotations for an audio recording, the MIDI file must first be aligned in time to the recording.  The reliability of the annotations therefore depends both on the quality of the MIDI file and the quality of the alignment scheme.  To quantify this, we compared aligned MIDI-derived beat annotations to Isophonics and [`madmom`](https://github.com/CPJKU/madmom)'s `DBNBeatTracker`, a start-of-the-art beat tracker which, on the Beatles data, can produce near-perfect annotations.  We will be using the alignment scheme proposed in [_\"Optimizing DTW-Based Audio-to-MIDI Alignment and Matching\"_](http://colinraffel.com/publications/icassp2016optimizing.pdf), which was found through a large-scale search over possible alignment systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CQT parameters\n",
    "NOTE_START = 36\n",
    "N_NOTES = 48\n",
    "\n",
    "def compute_cqt(audio_data, fs):\n",
    "    \"\"\"Compute some audio data's constant-Q spectrogram, normalize,\n",
    "    and log-scale it\"\"\"\n",
    "    # Compute CQT of the synthesized audio data\n",
    "    gram = librosa.cqt(\n",
    "        audio_data, fmin=librosa.midi_to_hz(NOTE_START), n_bins=N_NOTES)\n",
    "    # Compute log amplitude\n",
    "    gram = librosa.logamplitude(gram, ref_power=gram.max())\n",
    "    # Transpose so that rows are samples\n",
    "    gram = gram.T\n",
    "    # and L2 normalize\n",
    "    return librosa.util.normalize(gram, norm=2., axis=1)\n",
    "\n",
    "def frame_times(gram, fs=22050, hop_length=512):\n",
    "    \"\"\"Get the times corresponding to the frames in a spectrogram, which was\n",
    "    created with the default parameters of librosa.cqt\"\"\"\n",
    "    return librosa.frames_to_time(np.arange(gram.shape[0]), fs, hop_length)\n",
    "\n",
    "def evaluate_midi(midi_file, audio_file, annotation_file):\n",
    "    \"\"\"Align a MIDI file to an audio recording, extract beat \n",
    "    annotations, and compare them to handmdae ground truth.\"\"\"\n",
    "    # Load in audio data\n",
    "    audio, fs = librosa.load(audio_file)\n",
    "    # Load in MIDI data\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "    # Synthesize MIDI\n",
    "    midi_audio = pm.fluidsynth(fs)\n",
    "    # Copute log-magnitude, L2-normalized, constant-Q spectrograms\n",
    "    audio_gram = compute_cqt(audio, fs)\n",
    "    midi_gram = compute_cqt(midi_audio, fs)\n",
    "    # Get distance matrix\n",
    "    distance_matrix = 1 - np.dot(midi_gram, audio_gram.T)\n",
    "    # Non-diagonal additive path penalty is the median of the sim mtx\n",
    "    add_pen = np.median(distance_matrix)\n",
    "    # Get best path through matrix\n",
    "    aligned_midi_indices, aligned_audio_indices, confidence_score = djitw.dtw(\n",
    "        distance_matrix, gully=.96, additive_penalty=add_pen,\n",
    "        inplace=False)\n",
    "    # Normalize score by path length\n",
    "    confidence_score /= float(len(aligned_midi_indices))\n",
    "    # Normalize score by score by mean sim matrix value within path chunk\n",
    "    confidence_score /= distance_matrix[\n",
    "        aligned_midi_indices.min():aligned_midi_indices.max(),\n",
    "        aligned_audio_indices.min():aligned_audio_indices.max()].mean()\n",
    "    # The confidence score is a normalized DTW distance, which\n",
    "    # approximately follows in the range [.5, 1.] with .5 meaning a very\n",
    "    # good alignment.  This maps the scores from [0., 1.] where 1. means a\n",
    "    # very good alignment.\n",
    "    confidence_score = np.clip(2*(1 - confidence_score), 0, 1)\n",
    "    # Adjust MIDI timing\n",
    "    midi_frame_times = frame_times(midi_gram)\n",
    "    audio_frame_times = frame_times(audio_gram)\n",
    "    pm.adjust_times(midi_frame_times[aligned_midi_indices],\n",
    "                    audio_frame_times[aligned_audio_indices])\n",
    "    # Get beats from aligned MIDI\n",
    "    midi_beats = pm.get_beats()\n",
    "    # Get beats from the Isophonics file\n",
    "    reference_beats, _ = mir_eval.io.load_labeled_events(annotation_file)\n",
    "    # Get beat evaluation scores\n",
    "    beat_scores = mir_eval.beat.evaluate(reference_beats, midi_beats)\n",
    "    return [confidence_score, beat_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve list of MIDI files by globbing\n",
    "midi_files = glob.glob(os.path.join('data', 'mid', '*', '*.mid*'))\n",
    "# Assemble audio file list by replacing mid with wav\n",
    "audio_files = [os.path.splitext(f.replace('mid', 'wav'))[0] + '.wav'\n",
    "               for f in midi_files]\n",
    "# Same, replacing with isophonics and .txt\n",
    "annotation_files = [os.path.splitext(f.replace('mid', 'isophonics'))[0] + '.txt'\n",
    "                    for f in midi_files]\n",
    "# Retrieve all scores using joblib in parallel for speed\n",
    "score_list = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(evaluate_midi)(*args)\n",
    "    for args in zip(midi_files, audio_files, annotation_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in DBNBeatTracker annotations for comparison\n",
    "# They can be generated by running\n",
    "#    mkdir dbn_annotations; DBNBeatTracker batch wav/*/*.wav -o dbn_annotations/\n",
    "# and then un-flattening the directory structure and renaming .beats.txt to .txt\n",
    "dbn_scores = []\n",
    "# Get a list of all of the DBNBeatTracker output files\n",
    "dbn_files_glob = glob.glob(os.path.join('data', 'dbn_annotations', '*', '*.txt'))\n",
    "for dbn_file in dbn_files_glob:\n",
    "    # Get isophonics ground truth filename path\n",
    "    ground_truth_beats_file = dbn_file.replace('dbn_annotations', 'isophonics')\n",
    "    # Skip in case Isophonics did not annotate this file\n",
    "    if not os.path.exists(ground_truth_beats_file):\n",
    "        continue\n",
    "    # Load in reference beats and estimated beats\n",
    "    ref_beats, _ = mir_eval.io.load_labeled_events(ground_truth_beats_file)\n",
    "    est_beats = mir_eval.io.load_events(dbn_file)\n",
    "    # Compute beat evaluation metrics\n",
    "    dbn_scores.append(mir_eval.beat.evaluate(ref_beats, est_beats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array of confidence scores output by evaluate_midi function\n",
    "confidence_scores = np.array([s[0] for s in score_list])\n",
    "# Plot these three metrics\n",
    "for n, metric in enumerate(['F-measure', 'Any Metric Level Total', 'Information gain']):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Get metric scores achieved by MIDI files\n",
    "    midi_beat_scores = np.array([s[1][metric] for s in score_list])\n",
    "    # Plot confidence on x axis, metric score in y axis\n",
    "    points = plt.scatter(confidence_scores, midi_beat_scores, c='#3778bf', alpha=.3)\n",
    "    # Also plot line showing the DBNBeatTracker mean score\n",
    "    dbn_mean = np.mean([s[metric] for s in dbn_scores])\n",
    "    plt.plot([-1, 2.], [dbn_mean, dbn_mean], 'k:', lw=4)\n",
    "    plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "    plt.title(metric)\n",
    "    plt.xlabel('Confidence score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots, each blue point corresponds to a MIDI file.  We don't expect every MIDI file to produce valid beat annotations (for example, if it is a bad transcription or the alignment failed), so we plot both the alignment confidence score (on the x-axis) and the beat evaluation metric score (on the y-axis).  Ideally, we'd like all points to be clustered in the bottom left (corresponding to bad annotations with low confidence scores) or top right (successful alignments with high confidence scores).  While this pattern does show up to some extent, there clearly are many MIDI files which produced poor annotations and had high confidence scores.  This problem is exacerbated by the fact that beat evaluation metrics are sensitive to small-scale timing errors and differences in metric level, which are both common issues in aligned MIDI files.  We therefore propose that **developing more precise MIDI-to-audio schemes with better confidence score reporting** will help leverage meter information from MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
