{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test whether audio files are valid by listening to the beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, fs = librosa.load(\"data/wav/10CD1_-_The_Beatles/CD1_-_06_-_The_Continuing_Story_of_Bungalow_Bill.wav\", sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beats, positions = mir_eval.io.load_time_series('data/isophonics/10CD1_-_The_Beatles/CD1_-_06_-_The_Continuing_Story_of_Bungalow_Bill.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicks = mir_eval.sonify.clicks(beats, fs=fs, length=a.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio([a, clicks], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create whoosh index for Isophonics .wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beatles_list = []\n",
    "# Iterate over all wav files\n",
    "for n, wav_file in enumerate(glob.glob(os.path.join('data', 'wav', '*', '*.wav'))):\n",
    "    # Reconstruct title from filename\n",
    "    filename = os.path.splitext(os.path.split(wav_file)[1])[0]\n",
    "    # Remove number (e.g. 01_-_) and replace underscores with spaces\n",
    "    title = re.split('[0-9][0-9]_-_', filename)[-1].replace('_', ' ')\n",
    "    # Construct path prefix\n",
    "    path = os.path.join(os.path.split(os.path.split(wav_file)[0])[1], filename)\n",
    "    # Add an entry for this file\n",
    "    beatles_list.append(\n",
    "        {'id': unicode(n), 'artist': u\"The Beatles\",\n",
    "         'title': unicode(title), 'path': unicode(path)})\n",
    "# Create the whoosh index\n",
    "whoosh_search.create_index(\n",
    "    os.path.join('data', 'index'), beatles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FIXES = {\"When I'm 64\": \"When I'm Sixty-Four\",\n",
    " \"I Want You (She's So Heavy)\": \"I Want You\",\n",
    " \"Blackbird\": \"Black Bird\"}\n",
    "def fix(s):\n",
    "    if s in FIXES:\n",
    "        return FIXES[s]\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path to clean_midi dataset\n",
    "CLEAN_MIDI_PATH = '/home/craffel/projects/midi-dataset/data/clean_midi/'\n",
    "# Get list of all MIDI file metadata\n",
    "index = whoosh_search.get_whoosh_index(os.path.join(CLEAN_MIDI_PATH, 'index'))\n",
    "with index.searcher() as searcher:\n",
    "    midi_list = list(searcher.documents())\n",
    "# Load in beatles index for searching\n",
    "beatles_index = whoosh_search.get_whoosh_index(\n",
    "    os.path.join('data', 'index'))\n",
    "with beatles_index.searcher() as searcher:\n",
    "    # Get list of beatles entries; we will use this to retrieve paths.\n",
    "    beatles_list = list(searcher.documents())\n",
    "    for entry in midi_list:\n",
    "        # Only search Beatles tracks\n",
    "        if entry['artist'] == 'The Beatles':\n",
    "            # Search the beatles index for this track\n",
    "            matches = whoosh_search.search(searcher, beatles_index.schema, entry['artist'], fix(entry['title']), 9)\n",
    "            # If we have this beatles track in the index\n",
    "            if len(matches) > 0:\n",
    "                # Grab the base path for this track\n",
    "                path = [e['path'] for e in beatles_list if e['id'] == matches[0][0]][0]\n",
    "                # Figure out how many versions of this Beatles song we already have copied\n",
    "                n = 0\n",
    "                while os.path.exists(os.path.join('data/mid', path + '.mid{}'.format(n))):\n",
    "                    n += 1\n",
    "                # Construct path to original file\n",
    "                orig_path = os.path.join(CLEAN_MIDI_PATH, 'mid', entry['path'] + '.mid')\n",
    "                # Try loading in this MIDI file\n",
    "                try:\n",
    "                    pretty_midi.PrettyMIDI(orig_path)\n",
    "                # If we can't load it, don't copy\n",
    "                except Exception as e:\n",
    "                    print \"{}\".format(e)\n",
    "                    continue\n",
    "                # Construct output path\n",
    "                output_path = os.path.join('data', 'mid', path + '.mid{}'.format(n))\n",
    "                # Create output path if it doesn't exist\n",
    "                if not os.path.exists(os.path.split(output_path)[0]):\n",
    "                    os.makedirs(os.path.split(output_path)[0])\n",
    "                # Copy the MIDI file\n",
    "                shutil.copy(orig_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The frame resolution used in align_text_matches is 1024 samples\n",
    "# At 22.05 kHz this corresponds to about 46 ms, which is around the\n",
    "# same temporal tolerance as beat tracking eval.  So, divide by 2\n",
    "# to make the temporal resolution finer.\n",
    "align_text_matches.feature_extraction.AUDIO_HOP = 512\n",
    "align_text_matches.feature_extraction.MIDI_HOP = 256\n",
    "# Also need to change it in feature_extraction\n",
    "feature_extraction.AUDIO_HOP = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('data', 'mid_aligned')):\n",
    "    os.makedirs(os.path.join('data', 'mid_aligned'))\n",
    "if not os.path.exists(os.path.join('data', 'diagnostics')):\n",
    "    os.makedirs(os.path.join('data', 'diagnostics'))\n",
    "pairs = []\n",
    "# Construct pairs\n",
    "for midi_filename in glob.glob('data/mid/*/*.mid*'):\n",
    "    path, midi_filename_only = os.path.split(midi_filename)\n",
    "    midi_path = os.path.join(os.path.split(path)[1], midi_filename_only)\n",
    "    audio_filename = os.path.join(\n",
    "        'data', 'wav', os.path.splitext(midi_path)[0] + '.wav')\n",
    "    audio_features_filename = os.path.join(\n",
    "        'data', 'wav', os.path.splitext(midi_path)[0] + '.h5')\n",
    "    midi_features_filename = os.path.join(\n",
    "        'data', 'mid', midi_path.replace('.mid', '.h5'))\n",
    "    output_midi_filename = os.path.join(\n",
    "        'data', 'mid_aligned', midi_path)\n",
    "    output_diagnostics_filename = os.path.join(\n",
    "        'data', 'diagnostics', midi_path.replace('.mid', '.h5'))\n",
    "    pairs.append((audio_filename, midi_filename, audio_features_filename,\n",
    "                  midi_features_filename, output_midi_filename,\n",
    "                  output_diagnostics_filename))\n",
    "\n",
    "# Run alignment\n",
    "_ = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(align_text_matches.align_one_file)(*args)\n",
    "    for args in pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolate_times(times, old_timebase, new_timebase, labels=None,\n",
    "                      shift_start=False):\n",
    "    '''\n",
    "    Linearly interpolate a set of times (and optionally labels) to a new\n",
    "    timebase.  All returned times will fall within the range of\n",
    "    ``new_timebase``, and only times which fall within ``old_timebase`` will be\n",
    "    interpolated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - times : np.ndarray\n",
    "        Times of some events to be interpolated.\n",
    "    - old_timebase : np.ndarray\n",
    "        The original timebase of ``times``.\n",
    "    - new_timebase : np.ndarray\n",
    "        The new timebase to resample ``times`` to.\n",
    "    - labels : list or NoneType\n",
    "        Labels of the events in ``times``; if ``None``, no interpolated labels\n",
    "        will be generated.\n",
    "    - shift_start : bool\n",
    "        Whether to create an additional interpolated event with time\n",
    "        ``new_timebase[0]`` when any entry of ``times`` is before\n",
    "        ``old_timebase[0]`` and ``new_timebase[0]``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - interpolated_times : np.ndarray\n",
    "        Interpolated times.\n",
    "    - interpolated_labels : list\n",
    "        Interpolated labels.  Only returned when ``labels`` is not ``None``.\n",
    "    '''\n",
    "    # Remove all times which fall outside of the range of the original timebase\n",
    "    valid_times = [time for time in times\n",
    "                   if (time >= old_timebase[0]\n",
    "                       and time <= old_timebase[-1])]\n",
    "    # When labels are provided, also remove labels whose time falls outside of\n",
    "    # the range of the original timebase\n",
    "    if labels is not None:\n",
    "        valid_labels = [label for (time, label) in zip(times, labels)\n",
    "                        if (time >= old_timebase[0]\n",
    "                            and time <= old_timebase[-1])]\n",
    "    # Linearly interpolate the provided times to the new timebase\n",
    "    interped_times = np.interp(valid_times, old_timebase, new_timebase)\n",
    "    # If we have been told to add a time when an event falls before the\n",
    "    # timebases...\n",
    "    if (shift_start and np.any(times < new_timebase[0])\n",
    "            and np.any(times < old_timebase[0])\n",
    "            and not np.any(times == old_timebase[0])):\n",
    "        # Add an event at the beginning of the new timebase\n",
    "        interped_times = np.append(new_timebase[0], interped_times)\n",
    "        # If labels were provided, find the label of the first event before\n",
    "        # the old timebase and add it to the output labels\n",
    "        if labels is not None:\n",
    "            first_label = np.argmin(times < old_timebase[0]) - 1\n",
    "            valid_labels = [labels[first_label]] + valid_labels\n",
    "    # When labels were not provided, just return interpolated times\n",
    "    if labels is None:\n",
    "        return interped_times\n",
    "    # When labels were provided, return interpolated times and labels\n",
    "    else:\n",
    "        return interped_times, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extraction.AUDIO_HOP = 512\n",
    "errors = np.zeros(1000)\n",
    "def get_error():\n",
    "    fs = feature_extraction.AUDIO_FS\n",
    "    s = np.zeros((fs*np.random.randint(1, 5)))\n",
    "    place = np.random.randint(0, s.size)\n",
    "    s[place] = 1\n",
    "    gram = librosa.cqt(s, sr=fs, hop_length=feature_extraction.AUDIO_HOP,\n",
    "                       fmin=librosa.midi_to_hz(feature_extraction.NOTE_START),\n",
    "                       n_bins=feature_extraction.N_NOTES).T\n",
    "    return place/22050. - librosa.frames_to_time(\n",
    "        np.argmax(gram.sum(axis=1)),\n",
    "        hop_length=feature_extraction.AUDIO_HOP,\n",
    "        sr=feature_extraction.AUDIO_FS)[0]\n",
    "    #return place/22050. - feature_extraction.frame_times(gram)[np.argmax(gram.sum(axis=1))]\n",
    "errors = _ = joblib.Parallel(n_jobs=10, verbose=0)(\n",
    "    joblib.delayed(get_error)() for _ in range(1000))\n",
    "_ = plt.hist(errors, bins=20)\n",
    "print np.mean(errors), np.median(errors), np.max(errors) - np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('data', 'extracted')):\n",
    "    os.makedirs(os.path.join('data', 'extracted'))\n",
    "\n",
    "def process_one_file(diagnostics_file):\n",
    "    diagnostics = deepdish.io.load(diagnostics_file)\n",
    "    # Load the extracted features\n",
    "    midi_features = deepdish.io.load(diagnostics['midi_features_filename'])\n",
    "    audio_features = deepdish.io.load(\n",
    "        diagnostics['audio_features_filename'])\n",
    "    # Load in the original MIDI file\n",
    "    midi_object = pretty_midi.PrettyMIDI(str(diagnostics['midi_filename']))\n",
    "    # Compute the times of the frames (will be used for interpolation)\n",
    "    midi_frame_times = feature_extraction.frame_times(\n",
    "        midi_features['gram'])[diagnostics['aligned_midi_indices']]\n",
    "    audio_frame_times = feature_extraction.frame_times(\n",
    "        audio_features['gram'])[diagnostics['aligned_audio_indices']]\n",
    "    adjusted_beats = interpolate_times(\n",
    "        midi_object.get_beats(), midi_frame_times, audio_frame_times)\n",
    "    output_file = diagnostics_file.replace('diagnostics', 'extracted').replace('.h5', '.txt')\n",
    "    if not os.path.exists(os.path.split(output_file)[0]):\n",
    "        os.makedirs(os.path.split(output_file)[0])\n",
    "    np.savetxt(output_file, adjusted_beats)\n",
    "\n",
    "_ = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(process_one_file)(diagnostics_file)\n",
    "    for diagnostics_file in glob.glob(\n",
    "        os.path.join('data', 'diagnostics', '*', '*.h5*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate extracted ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(glob.glob('data/isophonics/*/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test a single example\n",
    "ref_beats, ref_labels = mir_eval.io.load_time_series('data/isophonics/01_-_Please_Please_Me/01_-_I_Saw_Her_Standing_There.txt')\n",
    "est_beats = mir_eval.io.load_events(\"data/extracted/01_-_Please_Please_Me/01_-_I_Saw_Her_Standing_There.txt0\")\n",
    "mir_eval.beat.evaluate(ref_beats, est_beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_reference_beat_variations(reference_beats):\n",
    "    # Create annotations at twice the metric level\n",
    "    interpolated_indices = np.arange(0, reference_beats.shape[0]-.5, .5)\n",
    "    original_indices = np.arange(0, reference_beats.shape[0])\n",
    "    double_reference_beats = np.interp(interpolated_indices,\n",
    "                                       original_indices,\n",
    "                                       reference_beats)\n",
    "    interpolated_indices = np.arange(0, reference_beats.shape[0]-.5, 1./3)\n",
    "    original_indices = np.arange(0, reference_beats.shape[0])\n",
    "    triple_reference_beats = np.interp(interpolated_indices,\n",
    "                                       original_indices,\n",
    "                                       reference_beats)\n",
    "    # Return metric variations:\n",
    "    # True, off-beat, double tempo, half tempo odd, and half tempo even\n",
    "    return (reference_beats,\n",
    "            double_reference_beats[1::2],\n",
    "            double_reference_beats,\n",
    "            reference_beats[::2],\n",
    "            reference_beats[1::2],\n",
    "            triple_reference_beats,\n",
    "            reference_beats[::3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(estimated_beats_file):\n",
    "    est_beats = mir_eval.io.load_events(estimated_beats_file)\n",
    "    if est_beats.size == 0:\n",
    "        return None, None\n",
    "    ground_truth_beats_file = os.path.splitext(estimated_beats_file.replace('extracted', 'isophonics'))[0] + '.txt'\n",
    "    ref_beats = mir_eval.io.load_labeled_events(ground_truth_beats_file)[0]\n",
    "    d = deepdish.io.load(estimated_beats_file.replace('extracted', 'diagnostics').replace('.txt', '.h5'))\n",
    "    confidence = d['score']\n",
    "    return confidence, mir_eval.beat.evaluate(ref_beats, est_beats)\n",
    "confidence_and_evaluation_scores = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(get_scores)(estimated_beats_file)\n",
    "    for estimated_beats_file in glob.glob(os.path.join('data', 'extracted', '*', '*.txt*')))\n",
    "confidence_scores = [e[0] for e in confidence_and_evaluation_scores if e[0] is not None]\n",
    "evaluation_scores = [e[1] for e in confidence_and_evaluation_scores if e[1] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "#import mpld3\n",
    "#import mpld3.plugins\n",
    "#mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, sharex=\"col\", sharey=\"row\", figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    points = ax[n / 3, n % 3].scatter(\n",
    "        confidence_scores,\n",
    "        np.array([s[metric] for s in evaluation_scores]),\n",
    "        c='#3778bf',\n",
    "        alpha=.3)\n",
    "    dbn_mean = np.mean([s[metric] for s in dbn_scores])\n",
    "    ax[n / 3, n % 3].plot([-1, 2.], [dbn_mean, dbn_mean], 'k:', lw=4)\n",
    "    dbn_std = np.std([s[metric] for s in dbn_scores])\n",
    "    ax[n / 3, n % 3].set_xlim([-.05, 1.05])\n",
    "    ax[n / 3, n % 3].set_ylim([-.05, 1.05])\n",
    "    ax[n / 3, n % 3].set_title(metric)\n",
    "#mpld3.plugins.connect(fig, mpld3.plugins.LinkedBrush(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=\"col\", sharey=\"row\", figsize=(12, 3))\n",
    "for n, metric in enumerate(['F-measure', 'Any Metric Level Total', 'Information gain']):\n",
    "    points = ax[n].scatter(\n",
    "        confidence_scores,\n",
    "        np.array([s[metric] for s in evaluation_scores]),\n",
    "        c='#3778bf',\n",
    "        alpha=.3)\n",
    "    dbn_mean = np.mean([s[metric] for s in dbn_scores])\n",
    "    ax[n].plot([-1, 2.], [dbn_mean, dbn_mean], 'k:', lw=4)\n",
    "    dbn_std = np.std([s[metric] for s in dbn_scores])\n",
    "    ax[n].set_xlim([-.05, 1.05])\n",
    "    ax[n].set_ylim([-.05, 1.05])\n",
    "    ax[n].set_title(metric)\n",
    "    if n == 0:\n",
    "        ax[n].set_ylabel('Metric score')\n",
    "    if n == 1:\n",
    "        ax[n].set_xlabel('Confidence score')\n",
    "plt.savefig('beat_scores.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here](beat_scores.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to beat tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce beat annotations using [madmom](https://github.com/CPJKU/madmom)'s \"DBNBeatTracker\" (general-purpose algorithm, SOTA in 2014, not trained on the Beatles) run the following from the data folder:\n",
    "\n",
    "`mkdir dbn_annotations; DBNBeatTracker batch wav/*/*.wav -o dbn_annotations/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DBNBeatTracker flattens directory structure and names things \".beats.txt\", so let's fix that\n",
    "for f in glob.glob('data/isophonics/*/*.txt'):\n",
    "    new_filename = f.replace('isophonics', 'dbn_annotations')\n",
    "    old_filename = os.path.join(\n",
    "        'data', 'dbn_annotations', os.path.splitext(os.path.split(f)[1])[0] + '.beats.txt')\n",
    "    if not os.path.exists(os.path.split(new_filename)[0]):\n",
    "        os.makedirs(os.path.split(new_filename)[0])\n",
    "    shutil.move(old_filename, new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract beats with librosa\n",
    "for f in glob.glob('data/wav/*/*.wav'):\n",
    "    audio_data, _ = librosa.load(f)\n",
    "    beats = librosa.frames_to_time(librosa.beat.beat_track(audio_data)[1])\n",
    "    output_beats_file = f.replace('.wav', '.txt').replace('wav', 'librosa_annotations')\n",
    "    if not os.path.exists(os.path.split(output_beats_file)[0]):\n",
    "        os.makedirs(os.path.split(output_beats_file)[0])\n",
    "    np.savetxt(output_beats_file, beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_dir = 'dbn_annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbn_scores = []\n",
    "for estimated_beats_file in glob.glob(os.path.join('data', compare_dir, '*', '*.txt')):\n",
    "    ground_truth_beats_file = estimated_beats_file.replace(compare_dir, 'isophonics')\n",
    "    if not os.path.exists(ground_truth_beats_file):\n",
    "        continue\n",
    "    ref_beats, _ = mir_eval.io.load_labeled_events(ground_truth_beats_file)\n",
    "    est_beats = mir_eval.io.load_events(estimated_beats_file)\n",
    "    dbn_scores.append(mir_eval.beat.evaluate(ref_beats, est_beats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, .95, 95, endpoint=False)\n",
    "fig, ax = plt.subplots(3, 3, sharex=\"col\", figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    metric_scores = np.array([s[metric] for s in evaluation_scores])\n",
    "    mean_acc = np.array([np.median(metric_scores[confidence_scores > t]) for t in thresholds])\n",
    "    p25_acc = np.array([np.percentile(metric_scores[confidence_scores > t], 25) for t in thresholds])\n",
    "    p75_acc = np.array([np.percentile(metric_scores[confidence_scores > t], 75) for t in thresholds])\n",
    "    ax[n / 3, n % 3].plot(thresholds, mean_acc, c='#3778bf', lw=2)\n",
    "    ax[n / 3, n % 3].fill_between(thresholds, p25_acc, p75_acc, facecolor='#35ad6b', alpha=.2)\n",
    "    ax[n / 3, n % 3].set_title(metric)\n",
    "    dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    ax[n / 3, n % 3].plot([0, 1.], [dbn_median, dbn_median], 'k:', lw=4)\n",
    "    if metric == 'Information gain':\n",
    "        ax[n / 3, n % 3].set_ylim(0, .65)\n",
    "    else:\n",
    "        ax[n / 3, n % 3].set_ylim(.5, 1)\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.08, 'Confidence score', ha='center')\n",
    "fig.text(0.08, 0.5, 'Metric score', va='center', rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.violinplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacing = np.linspace(0, 1, 21)\n",
    "fig, ax = plt.subplots(3, 3, sharex=True, figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    scores = []\n",
    "    my_spacing = []\n",
    "    for start, end in zip(spacing[:-1], spacing[1:]):\n",
    "        scores_in_range = [s for m, s in zip(evaluation_scores, confidence_scores)\n",
    "                           if m[metric] >= start and m[metric] < end]\n",
    "        if len(scores_in_range) > 1:\n",
    "            scores.append(scores_in_range)\n",
    "            my_spacing.append(start)\n",
    "    plt.sca(ax[n / 3, n % 3])\n",
    "    plt.violinplot(scores, my_spacing, widths=np.diff(spacing)[0], showextrema=False)\n",
    "    #dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    #plt.plot([0, 1], [dbn_median, dbn_median], 'k:', lw=2)\n",
    "    #dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    #plt.plot([dbn_median, dbn_median], [0, 2*max(n)], 'k:', lw=2)\n",
    "    plt.title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    metric_scores = np.array([s[metric] for s in evaluation_scores])\n",
    "    plt.sca(ax[n / 3, n % 3])\n",
    "    n, _, _ = plt.hist(metric_scores, np.linspace(0, 1, 21), facecolor='#35ad6b', alpha=.5, orientation='horizontal')\n",
    "    dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    plt.plot([0, 2*max(n)], [dbn_median, dbn_median], 'k:', lw=2)\n",
    "    plt.title(metric)\n",
    "    plt.xlim([0, max(n)*1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study particularly bad alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimated_beats_file = 'data/extracted/07_-_Revolver/02_-_Eleanor_Rigby.txt4'\n",
    "est_beats = mir_eval.io.load_events(estimated_beats_file)\n",
    "ground_truth_beats_file = os.path.splitext(estimated_beats_file.replace('extracted', 'isophonics'))[0] + '.txt'\n",
    "ref_beats = mir_eval.io.load_labeled_events(ground_truth_beats_file)[0]\n",
    "d = deepdish.io.load(estimated_beats_file.replace('extracted', 'diagnostics').replace('.txt', '.h5'))\n",
    "print mir_eval.beat.evaluate(ref_beats, est_beats)\n",
    "print d['score']\n",
    "\n",
    "#midi_object = pretty_midi.PrettyMIDI(str(d['output_midi_filename']))\n",
    "#m = midi_object.fluidsynth(22050)\n",
    "#a, fs = librosa.load(str(d['audio_filename']))\n",
    "#IPython.display.Audio([a, mir_eval.sonify.clicks(est_beats, 22050, length=a.shape[0])], rate=22050)\n",
    "plt.figure()\n",
    "plt.plot(np.diff(d['aligned_midi_indices']))\n",
    "plt.ylim([-.1, 1.1])\n",
    "plt.figure()\n",
    "plt.plot(np.diff(d['aligned_audio_indices']))\n",
    "plt.ylim([-.1, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d, metrics, ref_b, est_b in zip(diagnostics, evaluation_scores, ref_beats, est_beats):\n",
    "    if d['score'] > .6 and metrics['Information gain'] < .05:\n",
    "        audio_filename = d['audio_filename']\n",
    "        audio, fs = librosa.load(d['audio_filename'])\n",
    "        ref_clicks = mir_eval.sonify.clicks(\n",
    "            ref_b, fs, length=audio.shape[0])\n",
    "        est_clicks = mir_eval.sonify.clicks(\n",
    "            est_b, fs, length=audio.shape[0])\n",
    "        IPython.display.display(IPython.display.Audio([ref_clicks + audio, est_clicks + audio], rate=fs))\n",
    "        fig = plt.figure(figsize=(15, .5))\n",
    "        dist = np.min(np.abs(np.subtract.outer(est_b, ref_b)), axis=1)\n",
    "        plt.scatter(est_b, np.zeros(len(est_b)), c=dist, vmin=0, vmax=.25, alpha=.5, lw=0, cmap=plt.cm.jet)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import alignment_analysis\n",
    "midi_object = pretty_midi.PrettyMIDI(str(d['output_midi_filename']))\n",
    "IPython.display.Audio(alignment_analysis.synthesize_aligned_midi(audio, fs, midi_object), rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import djitw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio_gram = deepdish.io.load(str(d['audio_features_filename']))['gram']\n",
    "midi_gram = deepdish.io.load(str(d['midi_features_filenameusigram']\n",
    "D = 1 - np.dot(midi_gram, audio_gram.T)\n",
    "p, q, score = djitw.dtw(D, .96, np.median(D), inplace=False)\n",
    "mask = np.zeros_like(D)\n",
    "djitw.band_mask(.1, mask)\n",
    "print score/len(p)\n",
    "print score/len(p)/(np.sum(D*mask)/mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Key detection experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "import glob\n",
    "import pretty_midi\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import librosa\n",
    "import vamp\n",
    "import deepdish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key loading/computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_midi_key(filename):\n",
    "    ''' Load in key labels from a MIDI file '''\n",
    "    # Load in MIDI object and grab key change events\n",
    "    pm = pretty_midi.PrettyMIDI(filename)\n",
    "    key_changes = pm.key_signature_changes\n",
    "    # Convert each key change's number to a string (like 'C Major')\n",
    "    # Also convert it to lowercase, for mir_eval's sake\n",
    "    return [pretty_midi.key_number_to_key_name(k.key_number).lower()\n",
    "            for k in key_changes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_isophonics_key(filename):\n",
    "    ''' Read in key labels from an isophonics lab file '''\n",
    "    # Isophonics key lab files have three columns:\n",
    "    # start time, end time, and label\n",
    "    start, end, labels = mir_eval.io.load_delimited(\n",
    "        filename, [float, float, str])\n",
    "    # Extract key labels, which in lab files are formatted as\n",
    "    # 'key\\tC' or 'key\\tC:minor'\n",
    "    keys = [l.split('\\t')[1] for l in labels if 'Key' in l]\n",
    "    # Convert from 'C' and 'C:minor' to 'c major' and 'c minor'\n",
    "    for n, key in enumerate(keys):\n",
    "        if 'minor' in key:\n",
    "            keys[n] = key.replace(':', ' ').lower()\n",
    "        else:\n",
    "            keys[n] = key.lower() + ' major'\n",
    "        # Validate the key early\n",
    "        mir_eval.key.validate_key(keys[n])\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_vamp_key(filename):\n",
    "    ''' Estimate the key from an audio file using QM key detector '''\n",
    "    # Load in audio data at its native sampling rate\n",
    "    audio_data, fs = librosa.load(filename, sr=None)\n",
    "    # Create a vamp processor that will generate key labels\n",
    "    key_generator = vamp.process_audio_multiple_outputs(\n",
    "        audio_data, fs, 'qm-vamp-plugins:qm-keydetector', ['key'])\n",
    "    # Grab the key labels produced by the vampplugin\n",
    "    vamp_output = [out['key'] for out in key_generator]\n",
    "    keys = [l['label'] for l in vamp_output]\n",
    "    # Compute the durations of each key in the song\n",
    "    starts = [float(l['timestamp']) for l in vamp_output]\n",
    "    starts.append(librosa.get_duration(audio_data, fs))\n",
    "    durations = np.diff(starts)\n",
    "    unique_keys = list(set(keys))\n",
    "    key_durations = [sum(d for k, d in zip(keys, durations) if k == key)\n",
    "                     for key in unique_keys]\n",
    "    # Retrieve the key which spanned the most of the song\n",
    "    most_common_key = unique_keys[np.argmax(key_durations)]\n",
    "    # Sometimes vamp produces keys like\n",
    "    # 'Eb / D# minor'\n",
    "    # so here, we are just retrieving the last part ('D# minor')\n",
    "    if ' / ' in most_common_key:\n",
    "        most_common_key = most_common_key.split(' / ')[1]\n",
    "    return most_common_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of different key estimations/annotations compared to isophonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file for each source\n",
    "scores = collections.defaultdict(list)\n",
    "# Keep track of whether each MIDI estimated key is C major\n",
    "c_majors = []\n",
    "for lab_filename in glob.glob('data/isophonics_key/*/*.lab'):\n",
    "    # Load Isophonics key from .lab file\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(lab_filename)\n",
    "    except Exception as e:\n",
    "        # Keep track of how many isophonics files which have invalid keys\n",
    "        print 'Error for {}: {}'.format(lab_filename, e)\n",
    "        n_skipped['isophonics_invalid'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['>1_isophonics_keys'] += 1\n",
    "        continue\n",
    "    isophonics_key = isophonics_keys[0]\n",
    "    \n",
    "    # Loop over all possible MIDI files for this key\n",
    "    midi_glob = lab_filename.replace('isophonics_key', 'mid').replace('.lab', '.mid*')\n",
    "    for midi_filename in glob.glob(midi_glob):\n",
    "        # Get keys from MIDI file\n",
    "        try:\n",
    "            midi_keys = load_midi_key(midi_filename)\n",
    "        except Exception as e:\n",
    "            print 'Error for {}: {}'.format(midi_filename, e)\n",
    "            n_skipped['midi_exceptions'] += 1\n",
    "            continue\n",
    "        # If there's no key change event, skip\n",
    "        if len(midi_keys) == 0:\n",
    "            n_skipped['no_midi_keys'] += 1\n",
    "            continue\n",
    "        # If there's multiple key change events, skip\n",
    "        if len(midi_keys) > 1:\n",
    "            n_skipped['>1_midi_keys'] += len(midi_keys) > 1\n",
    "            continue\n",
    "        midi_key = midi_keys[0]\n",
    "        # Keep track of whether the estimated key was a C major\n",
    "        c_majors.append(midi_keys[0] == 'c major')\n",
    "        # Compute and store score for this MIDI file\n",
    "        scores['midi'].append(mir_eval.key.weighted_score(isophonics_key, midi_key))\n",
    "\n",
    "    # Construct .wav filename from .lab filename\n",
    "    audio_filename = lab_filename.replace('isophonics_key', 'wav').replace('.lab', '.wav')\n",
    "    # Estimate the key using vamp QM key detector plugin\n",
    "    try:\n",
    "        vamp_key = load_vamp_key(audio_filename)\n",
    "    except Exception as e:\n",
    "        print 'Error for {}: {}'.format(audio_filename, e)\n",
    "        n_skipped['audio_exceptions'] += 1\n",
    "        continue\n",
    "    scores['vamp'].append(mir_eval.key.weighted_score(isophonics_key, vamp_key))\n",
    "\n",
    "    # Construct whatkeyisitin text filename from .lab filename\n",
    "    whatkeyisitin_filename = lab_filename.replace('isophonics_key', 'whatkeyisitin_key').replace('.lab', '.txt')\n",
    "    if not os.path.exists(whatkeyisitin_filename):\n",
    "        # Keep track of how many are skipped due to missing wkiii annotation\n",
    "        n_skipped['no_wkiii_file'] += 1\n",
    "        continue\n",
    "    with open(whatkeyisitin_filename) as f:\n",
    "        whatkeyisitin_key = f.read()\n",
    "    scores['wkiii'].append(mir_eval.key.weighted_score(isophonics_key, whatkeyisitin_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print statistics about the MIDI key accuracy\n",
    "for key, value in n_skipped.items():\n",
    "    print '{} skipped because {}'.format(value, key)\n",
    "print 'Total isophonics .lab files: {}'.format(len(glob.glob('data/isophonics_key/*/*.lab')))\n",
    "print\n",
    "mean_scores = collections.OrderedDict([\n",
    "    ('MIDI, all keys', np.mean(scores['midi'])),\n",
    "    ('MIDI, C major only', np.mean([s for c, s in zip(c_majors, scores['midi']) if c])),\n",
    "    ('MIDI, non-C major', np.mean([s for c, s in zip(c_majors, scores['midi']) if not c])),\n",
    "    ('QM Key Detector', np.mean(scores['vamp'])),\n",
    "    ('whatkeyisitin.com', np.mean(scores['wkiii']))])\n",
    "n_comparisons = collections.OrderedDict([\n",
    "    ('MIDI, all keys', len(scores['midi'])),\n",
    "    ('MIDI, C major only', sum(c_majors)),\n",
    "    ('MIDI, non-C major', len([c for c in c_majors if not c])),\n",
    "    ('QM Key Detector', len(scores['vamp'])),\n",
    "    ('whatkeyisitin.com', len(scores['wkiii']))])\n",
    "print tabulate.tabulate(\n",
    "    [(name, score, num) for (name, score, num) in \n",
    "     zip(mean_scores.keys(), mean_scores.values(), n_comparisons.values())],\n",
    "    ['Source', 'Mean score', '# of comparisons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(len(s['key_numbers']) for s in statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([k for k in sum([s['key_numbers'] for s in statistics], []) if k == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "ts = np.linspace(0, 1, 101)\n",
    "a = [np.mean([s for c, s, a in zip(c_majors, scores, alignment_scores) if not c and a > t]) for t in ts]\n",
    "plt.plot(ts, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of vamp keys compared to isophonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file\n",
    "scores = []\n",
    "for audio_filename in glob.glob('data/wav/*/*.wav'):\n",
    "\n",
    "    \n",
    "    # Construct .lab file path from .wav file path\n",
    "    base_path, filename = os.path.split(audio_filename)\n",
    "    lab_filename = os.path.join(base_path.replace('wav', 'isophonics_key'),\n",
    "                                os.path.splitext(filename)[0] + '.lab')\n",
    "    if not os.path.exists(lab_filename):\n",
    "        n_skipped['no_lab_file'] += 1\n",
    "        continue\n",
    "    # Load in Isophonics keys from .lab file\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(lab_filename)\n",
    "    except Exception as e:\n",
    "        print 'Error for {}: {}'.format(lab_filename, e)\n",
    "        n_skipped['isophonics_exceptions'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['>1_isophonics_keys'] += 1\n",
    "        continue\n",
    "    # Compute and store score for this wav file\n",
    "    scores.append(mir_eval.key.weighted_score(isophonics_keys[0], vamp_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total possible: {}'.format(len(glob.glob('data/wav/*/*.wav')))\n",
    "print '# of audio exceptions: {}'.format(n_skipped['audio_exceptions'])\n",
    "print '# of Isophonics exceptions: {}'.format(n_skipped['isophonics_exceptions'])\n",
    "print '# of Isophonics with >1 keys: {}'.format(n_skipped['>1_isophonics_keys'])\n",
    "print '# of valid comparisons: {}'.format(len(scores))\n",
    "print 'Accuracy all: {:.3f}'.format(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/whatkeyisitin.txt') as f:\n",
    "    whatkeyisitin_list = [l.strip().split('\\t') for l in f]\n",
    "# Load in beatles index for searching\n",
    "beatles_index = whoosh_search.get_whoosh_index(\n",
    "    os.path.join('data', 'index'))\n",
    "with beatles_index.searcher() as searcher:\n",
    "    # Get list of beatles entries; we will use this to retrieve paths.\n",
    "    beatles_list = list(searcher.documents())\n",
    "    for (title, key) in whatkeyisitin_list:\n",
    "        # Search the beatles index for this track\n",
    "        matches = whoosh_search.search(searcher, beatles_index.schema, 'The Beatles', fix(title), 9)\n",
    "        # If we have this beatles track in the index\n",
    "        if len(matches) > 0:\n",
    "            # Grab the base path for this track\n",
    "            path = [e['path'] for e in beatles_list if e['id'] == matches[0][0]][0]\n",
    "            output_filename = os.path.join('data', 'whatkeyisitin_key', path + '.txt')\n",
    "            if not os.path.exists(os.path.split(output_filename)[0]):\n",
    "                os.makedirs(os.path.split(output_filename)[0])\n",
    "            with open(output_filename, 'wb') as f:\n",
    "                f.write(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file\n",
    "scores = []\n",
    "for whatkeyisitin_file in glob.glob('data/whatkeyisitin_key/*/*.txt'):\n",
    "    with open(whatkeyisitin_file) as f:\n",
    "        whatkeyisitin_key = f.read()\n",
    "    isophonics_file = whatkeyisitin_file.replace('whatkeyisitin_key', 'isophonics_key').replace('.txt', '.lab')\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(isophonics_file)\n",
    "    except Exception as e:\n",
    "        print 'Error for {}: {}'.format(lab_filename, e)\n",
    "        n_skipped['isophonics_exceptions'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['>1_isophonics_keys'] += 1\n",
    "        continue\n",
    "    scores.append(mir_eval.key.weighted_score(isophonics_keys[0], whatkeyisitin_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total possible: {}'.format(len(glob.glob('data/whatkeyisitin_key/*/*.txt')))\n",
    "print '# of Isophonics exceptions: {}'.format(n_skipped['isophonics_exceptions'])\n",
    "print '# of Isophonics with >1 keys: {}'.format(n_skipped['>1_isophonics_keys'])\n",
    "print '# of valid comparisons: {}'.format(len(scores))\n",
    "print 'Accuracy all: {:.3f}'.format(np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
