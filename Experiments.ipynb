{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import mir_eval\n",
    "import IPython\n",
    "import sys\n",
    "sys.path.append('/home/craffel/projects/midi-dataset/')\n",
    "sys.path.append('/home/craffel/projects/midi-dataset/scripts')\n",
    "import whoosh_search\n",
    "import os\n",
    "import shutil\n",
    "import pretty_midi\n",
    "import re\n",
    "import numpy as np\n",
    "import feature_extraction\n",
    "import deepdish\n",
    "import joblib\n",
    "import align_text_matches\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import matplotlib.gridspec\n",
    "import matplotlib\n",
    "import cPickle as pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute statistics about MIDI files from the wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_statistics(midi_file):\n",
    "    \"\"\"\n",
    "    Given a path to a MIDI file, compute a dictionary of statistics about it\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    midi_file : str\n",
    "        Path to a MIDI file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    statistics : dict\n",
    "        Dictionary reporting the program numbers, key numbers,\n",
    "        tempos, and time signatures present in the MIDI file.\n",
    "    \"\"\"\n",
    "    # Some MIDI files will raise Exceptions on loading, if they are invalid.\n",
    "    # We just skip those.\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "        # Extract informative events from the MIDI file\n",
    "        return {'n_instruments': len(pm.instruments),\n",
    "                'program_numbers': [i.program for i in pm.instruments if not i.is_drum],\n",
    "                'key_numbers': [k.key_number for k in pm.key_signature_changes],\n",
    "                'tempos': list(pm.get_tempo_changes()[1]),\n",
    "                'time_signature_changes': pm.time_signature_changes}\n",
    "    # Dear Python, sorry for the anti-pattern\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute statistics about every unique MIDI file we found on the internet\n",
    "statistics = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(compute_statistics)(midi_file)\n",
    "    for midi_file in glob.glob(os.path.join('data', 'unique_mid', '*', '*.mid')))\n",
    "# When an error occurred, None will be returned; filter those out.\n",
    "statistics = [s for s in statistics if s is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('statistics.pkl', 'wb') as f:\n",
    "    pickle.dump(statistics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('statistics.pkl') as f:\n",
    "    statistics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statistics = statistics[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('font', **{'size':13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BLUE = '#28ABE3'\n",
    "GREEN = '#1FDA9A'\n",
    "ORANGE = '#E8B71A'\n",
    "GREY = '#AAAAAA'\n",
    "\n",
    "def uniform_hist(data, bins, ax, **kwargs):\n",
    "    heights, _ = np.histogram(data, bins)\n",
    "    ax.bar(left=np.arange(len(bins) - 1) - .5, height=heights,\n",
    "           width=1, bottom=0, **kwargs)\n",
    "    return heights\n",
    "\n",
    "def pretty_hist(data, bins, fc, ax, title=None):\n",
    "    \"\"\" Utility method for plotting a nice histogram \"\"\"\n",
    "    # Make it so that all points beyond the bin range get put in the last bin\n",
    "    data = np.array(data)\n",
    "    data[data > bins[-1]] = bins[-1] - 1e-10\n",
    "    # Plot histogram, with specific coloring and axis-alignment\n",
    "    heights = uniform_hist(data, bins, ax, fc=fc, alpha=.7)\n",
    "    # Remove spines from plot\n",
    "    sns.despine()\n",
    "    # Add grid to y axis\n",
    "    ax.yaxis.grid()\n",
    "    # Set the plotting range to fit the histogram exactly\n",
    "    bin_spacing = 1.\n",
    "    ax.set_xlim(-bin_spacing/2., len(bins) - 1 - bin_spacing/2.)\n",
    "    if title is not None:\n",
    "        plt.suptitle(title, verticalalignment='top', y=.95, size='large')\n",
    "    return heights\n",
    "\n",
    "def divide_yticklabels(ax, divisor=1000):\n",
    "    \"\"\" Utility method to scale down all y tick labels \"\"\"\n",
    "    ax.set_yticklabels([int(float(t)/divisor)\n",
    "                        if (float(t)/divisor).is_integer()\n",
    "                        else float(t)/divisor\n",
    "                        for t in ax.get_yticks()])\n",
    "\n",
    "def split_hist(data, bin_edges, high_bin_indices, fc, title):\n",
    "    \"\"\" Plot a histogram where one or more bins have very large values \"\"\"\n",
    "    # Make high_bin_indices a list if an int was passed\n",
    "    if isinstance(high_bin_indices, int):\n",
    "        high_bin_indices = [high_bin_indices]\n",
    "    # Create 2-row, 1-col subplot where the upper sublot is 1/4 the height\n",
    "    # The upper subplot will be the tops of the very large bins; lower will be the rest\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 1, width_ratios=[1,], height_ratios=[1, 4])\n",
    "    # Set the spacing between subplots to .1\n",
    "    gs.update(hspace=0.1)\n",
    "    # Grab axes handles\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "    # Plot pretty histograms both for the \"upper\" and \"lower\" parts of the split\n",
    "    heights = pretty_hist(data, bin_edges, fc, ax)\n",
    "    pretty_hist(data, bin_edges, fc, ax2)\n",
    "    low_min = 0\n",
    "    # Compute the height of the largest bin _not_ in high_bin_indices\n",
    "    low_max = 1.1*max(heights[n] for n in range(len(bin_edges) - 1)\n",
    "                      if n not in high_bin_indices)\n",
    "    low_range = low_max - low_min\n",
    "    # Compute the height of the smallest bin in high_bin_indices\n",
    "    high_min = .9*min(heights[n] for n in high_bin_indices)\n",
    "    # Compute the height of the highest bin in high_bin_indices\n",
    "    high_max = 1.1*max(heights[n] for n in high_bin_indices)\n",
    "    # Set the Y plotting range according to the above.  This will crop things.\n",
    "    ax.set_ylim(high_min, high_max)\n",
    "    ax2.set_ylim(low_min, low_max)\n",
    "    # Hide the spines between ax and ax2\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.tick_params(labeltop='off')\n",
    "    ax2.xaxis.tick_bottom()\n",
    "\n",
    "    # Compute the spacing between y-ticks on the lower plot\n",
    "    lowtick_spacing = np.diff(ax2.get_yticks())[0]\n",
    "    # Create a single tick on the upper plot, rounded to the same spacing as lower plot\n",
    "    ax.set_yticks([int(lowtick_spacing)*int((high_min + high_max)/(2*lowtick_spacing))])\n",
    "\n",
    "    # X-axis start of clip lines (relative to [0, 1])\n",
    "    start = -.015\n",
    "    # Compute proportion of x-axis covered by last high_bin_indices (+ .015)\n",
    "    end = (high_bin_indices[-1] + 1)/float(len(bin_edges) - 1) + .015\n",
    "    # Plot the lines, allowing for it to expand outside of the axis\n",
    "    ax.plot([start, end], [0., 0.], transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax2.plot([start, end], [1., 1.], transform=ax2.transAxes, color='k', clip_on=False)\n",
    "\n",
    "    # Convert count to thousands\n",
    "    divide_yticklabels(ax)\n",
    "    divide_yticklabels(ax2)\n",
    "\n",
    "    plt.suptitle(title, verticalalignment='top', y=.95, size='large')\n",
    "\n",
    "plt.figure()\n",
    "pretty_hist([s['n_instruments'] for s in statistics],\n",
    "            range(22), BLUE, plt.gca(), '(a) Number of instruments')\n",
    "divide_yticklabels(plt.gca())\n",
    "plt.savefig('n_instruments.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.xticks(range(0, 22, 5), range(0, 22 - 5, 5) + ['20+'])\n",
    "\n",
    "plt.figure()\n",
    "split_hist([i for s in statistics for i in s['program_numbers']],\n",
    "           range(128), 0, BLUE, '(e) Program Numbers')\n",
    "plt.savefig('program_numbers.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "split_hist([len(s['tempos']) for s in statistics],\n",
    "           range(1, 12) + [30, 100, 1000], 0, GREEN, '(b) Number of tempo changes')\n",
    "plt.xticks(range(13), range(1, 11) + ['11 - 30', '31 - 100', '101+'], rotation=45, ha='center')\n",
    "plt.savefig('n_tempos.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.figure()\n",
    "pretty_hist([i for s in statistics for i in s['tempos']],\n",
    "            range(0, 260, 10), GREEN, plt.gca(), '(f) Tempos')\n",
    "divide_yticklabels(plt.gca())\n",
    "plt.xticks(range(0, len(range(0, 260, 10)), 3), range(0, 240, 30) + ['240+'], rotation=45)\n",
    "plt.savefig('tempos.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.figure()\n",
    "split_hist([len(s['key_numbers']) for s in statistics],\n",
    "           range(12), [0, 1], ORANGE, '(d) Number of key changes')\n",
    "plt.xticks(range(11), range(10) + ['10+'])\n",
    "plt.savefig('n_keys.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "fig = plt.figure()\n",
    "split_hist([i for s in statistics for i in s['key_numbers']],\n",
    "           range(25), 0, ORANGE, '(h) Keys')\n",
    "plt.xticks([0, 2, 4, 5, 7, 9, 11, 12, 14, 16, 17, 19, 21, 23],\n",
    "           ['C', 'D', 'E', 'F', 'G', 'A', 'B', 'c', 'd', 'e', 'f', 'g', 'a', 'b'])\n",
    "plt.figtext(0.28, .03, 'Major')\n",
    "plt.figtext(0.67, .03, 'Minor')\n",
    "l1 = matplotlib.lines.Line2D([.14, .26], [.045, .045], c='k', transform=fig.transFigure, figure=fig)\n",
    "l2 = matplotlib.lines.Line2D([.37, .49], [.045, .045], c='k', transform=fig.transFigure, figure=fig)\n",
    "l3 = matplotlib.lines.Line2D([.53, .65], [.045, .045], c='k', transform=fig.transFigure, figure=fig)\n",
    "l4 = matplotlib.lines.Line2D([.76, .88], [.045, .045], c='k', transform=fig.transFigure, figure=fig)\n",
    "fig.lines.extend([l1, l2, l3, l4])\n",
    "plt.savefig('keys.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.figure()\n",
    "split_hist([len(s['time_signature_changes']) for s in statistics],\n",
    "           range(12), 1, GREY, '(c) Number of time signature changes')\n",
    "plt.xticks(range(11), range(10) + ['10+'])\n",
    "plt.savefig('n_signatures.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Get strings for all time signatures\n",
    "time_signatures = ['{}/{}'.format(c.numerator, c.denominator)\n",
    "                   for s in statistics for c in s['time_signature_changes']]\n",
    "\n",
    "# Only display the n_top top time signatures\n",
    "n_top = 15\n",
    "# Get the n_top top time signatures\n",
    "top = collections.Counter(time_signatures).most_common(n_top)\n",
    "# Create a dict mapping an integer index to the time signature string\n",
    "top_signatures = {n: s[0] for n, s in enumerate(top)}\n",
    "# Add an additional index for non-top signatures\n",
    "top_signatures[n_top] = 'Other'\n",
    "# Compute the number of non-top time signatures\n",
    "n_other = len(time_signatures) - sum(s[1] for s in top)\n",
    "# Create a list with each index repeated the number of times\n",
    "# each time signature appears, to be passed to plt.hist\n",
    "indexed_time_signatures = sum([[n]*s[1] for n, s in enumerate(top)], [])\n",
    "indexed_time_signatures += [n_top]*n_other\n",
    "\n",
    "plt.figure()\n",
    "split_hist(indexed_time_signatures, range(n_top + 2), 0, GREY, '(g) Time signatures')\n",
    "plt.xticks(top_signatures.keys(), top_signatures.values(), rotation=45, ha='center')\n",
    "plt.savefig('time_signatures.pdf', transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_instruments = np.argsort(np.bincount([i for s in statistics for i in s['program_numbers']]))[-4:]\n",
    "print most_common_instruments\n",
    "print [pretty_midi.program_to_instrument_name(i) for i in most_common_instruments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a](n_instruments.pdf)\n",
    "\n",
    "[b](program_numbers.pdf)\n",
    "\n",
    "[c](n_tempos.pdf)\n",
    "\n",
    "[d](tempos.pdf)\n",
    "\n",
    "[e](n_keys.pdf)\n",
    "\n",
    "[f](keys.pdf)\n",
    "\n",
    "[g](n_signatures.pdf)\n",
    "\n",
    "[h](time_signatures.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test whether audio files are valid by listening to the beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, fs = librosa.load(\"data/wav/10CD1_-_The_Beatles/CD1_-_06_-_The_Continuing_Story_of_Bungalow_Bill.wav\", sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beats, positions = mir_eval.io.load_time_series('data/isophonics/10CD1_-_The_Beatles/CD1_-_06_-_The_Continuing_Story_of_Bungalow_Bill.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicks = mir_eval.sonify.clicks(beats, fs=fs, length=a.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio([a, clicks], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create whoosh index for Isophonics .wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beatles_list = []\n",
    "# Iterate over all wav files\n",
    "for n, wav_file in enumerate(glob.glob(os.path.join('data', 'wav', '*', '*.wav'))):\n",
    "    # Reconstruct title from filename\n",
    "    filename = os.path.splitext(os.path.split(wav_file)[1])[0]\n",
    "    # Remove number (e.g. 01_-_) and replace underscores with spaces\n",
    "    title = re.split('[0-9][0-9]_-_', filename)[-1].replace('_', ' ')\n",
    "    # Construct path prefix\n",
    "    path = os.path.join(os.path.split(os.path.split(wav_file)[0])[1], filename)\n",
    "    # Add an entry for this file\n",
    "    beatles_list.append(\n",
    "        {'id': unicode(n), 'artist': u\"The Beatles\",\n",
    "         'title': unicode(title), 'path': unicode(path)})\n",
    "# Create the whoosh index\n",
    "whoosh_search.create_index(\n",
    "    os.path.join('data', 'index'), beatles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FIXES = {\"When I'm 64\": \"When I'm Sixty-Four\",\n",
    " \"I Want You (She's So Heavy)\": \"I Want You\",\n",
    " \"Blackbird\": \"Black Bird\"}\n",
    "def fix(s):\n",
    "    if s in FIXES:\n",
    "        return FIXES[s]\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path to clean_midi dataset\n",
    "CLEAN_MIDI_PATH = '/home/craffel/projects/midi-dataset/data/clean_midi/'\n",
    "# Get list of all MIDI file metadata\n",
    "index = whoosh_search.get_whoosh_index(os.path.join(CLEAN_MIDI_PATH, 'index'))\n",
    "with index.searcher() as searcher:\n",
    "    midi_list = list(searcher.documents())\n",
    "# Load in beatles index for searching\n",
    "beatles_index = whoosh_search.get_whoosh_index(\n",
    "    os.path.join('data', 'index'))\n",
    "with beatles_index.searcher() as searcher:\n",
    "    # Get list of beatles entries; we will use this to retrieve paths.\n",
    "    beatles_list = list(searcher.documents())\n",
    "    for entry in midi_list:\n",
    "        # Only search Beatles tracks\n",
    "        if entry['artist'] == 'The Beatles':\n",
    "            # Search the beatles index for this track\n",
    "            matches = whoosh_search.search(searcher, beatles_index.schema, entry['artist'], fix(entry['title']), 9)\n",
    "            # If we have this beatles track in the index\n",
    "            if len(matches) > 0:\n",
    "                # Grab the base path for this track\n",
    "                path = [e['path'] for e in beatles_list if e['id'] == matches[0][0]][0]\n",
    "                # Figure out how many versions of this Beatles song we already have copied\n",
    "                n = 0\n",
    "                while os.path.exists(os.path.join('data/mid', path + '.mid{}'.format(n))):\n",
    "                    n += 1\n",
    "                # Construct path to original file\n",
    "                orig_path = os.path.join(CLEAN_MIDI_PATH, 'mid', entry['path'] + '.mid')\n",
    "                # Try loading in this MIDI file\n",
    "                try:\n",
    "                    pretty_midi.PrettyMIDI(orig_path)\n",
    "                # If we can't load it, don't copy\n",
    "                except Exception as e:\n",
    "                    print \"{}\".format(e)\n",
    "                    continue\n",
    "                # Construct output path\n",
    "                output_path = os.path.join('data', 'mid', path + '.mid{}'.format(n))\n",
    "                # Create output path if it doesn't exist\n",
    "                if not os.path.exists(os.path.split(output_path)[0]):\n",
    "                    os.makedirs(os.path.split(output_path)[0])\n",
    "                # Copy the MIDI file\n",
    "                shutil.copy(orig_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The frame resolution used in align_text_matches is 1024 samples\n",
    "# At 22.05 kHz this corresponds to about 46 ms, which is around the\n",
    "# same temporal tolerance as beat tracking eval.  So, divide by 2\n",
    "# to make the temporal resolution finer.\n",
    "align_text_matches.feature_extraction.AUDIO_HOP = 512\n",
    "align_text_matches.feature_extraction.MIDI_HOP = 256\n",
    "# Also need to change it in feature_extraction\n",
    "feature_extraction.AUDIO_HOP = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('data', 'mid_aligned')):\n",
    "    os.makedirs(os.path.join('data', 'mid_aligned'))\n",
    "if not os.path.exists(os.path.join('data', 'diagnostics')):\n",
    "    os.makedirs(os.path.join('data', 'diagnostics'))\n",
    "pairs = []\n",
    "# Construct pairs\n",
    "for midi_filename in glob.glob('data/mid/*/*.mid*'):\n",
    "    path, midi_filename_only = os.path.split(midi_filename)\n",
    "    midi_path = os.path.join(os.path.split(path)[1], midi_filename_only)\n",
    "    audio_filename = os.path.join(\n",
    "        'data', 'wav', os.path.splitext(midi_path)[0] + '.wav')\n",
    "    audio_features_filename = os.path.join(\n",
    "        'data', 'wav', os.path.splitext(midi_path)[0] + '.h5')\n",
    "    midi_features_filename = os.path.join(\n",
    "        'data', 'mid', midi_path.replace('.mid', '.h5'))\n",
    "    output_midi_filename = os.path.join(\n",
    "        'data', 'mid_aligned', midi_path)\n",
    "    output_diagnostics_filename = os.path.join(\n",
    "        'data', 'diagnostics', midi_path.replace('.mid', '.h5'))\n",
    "    pairs.append((audio_filename, midi_filename, audio_features_filename,\n",
    "                  midi_features_filename, output_midi_filename,\n",
    "                  output_diagnostics_filename))\n",
    "\n",
    "# Run alignment\n",
    "_ = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(align_text_matches.align_one_file)(*args)\n",
    "    for args in pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolate_times(times, old_timebase, new_timebase, labels=None,\n",
    "                      shift_start=False):\n",
    "    '''\n",
    "    Linearly interpolate a set of times (and optionally labels) to a new\n",
    "    timebase.  All returned times will fall within the range of\n",
    "    ``new_timebase``, and only times which fall within ``old_timebase`` will be\n",
    "    interpolated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - times : np.ndarray\n",
    "        Times of some events to be interpolated.\n",
    "    - old_timebase : np.ndarray\n",
    "        The original timebase of ``times``.\n",
    "    - new_timebase : np.ndarray\n",
    "        The new timebase to resample ``times`` to.\n",
    "    - labels : list or NoneType\n",
    "        Labels of the events in ``times``; if ``None``, no interpolated labels\n",
    "        will be generated.\n",
    "    - shift_start : bool\n",
    "        Whether to create an additional interpolated event with time\n",
    "        ``new_timebase[0]`` when any entry of ``times`` is before\n",
    "        ``old_timebase[0]`` and ``new_timebase[0]``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - interpolated_times : np.ndarray\n",
    "        Interpolated times.\n",
    "    - interpolated_labels : list\n",
    "        Interpolated labels.  Only returned when ``labels`` is not ``None``.\n",
    "    '''\n",
    "    # Remove all times which fall outside of the range of the original timebase\n",
    "    valid_times = [time for time in times\n",
    "                   if (time >= old_timebase[0]\n",
    "                       and time <= old_timebase[-1])]\n",
    "    # When labels are provided, also remove labels whose time falls outside of\n",
    "    # the range of the original timebase\n",
    "    if labels is not None:\n",
    "        valid_labels = [label for (time, label) in zip(times, labels)\n",
    "                        if (time >= old_timebase[0]\n",
    "                            and time <= old_timebase[-1])]\n",
    "    # Linearly interpolate the provided times to the new timebase\n",
    "    interped_times = np.interp(valid_times, old_timebase, new_timebase)\n",
    "    # If we have been told to add a time when an event falls before the\n",
    "    # timebases...\n",
    "    if (shift_start and np.any(times < new_timebase[0])\n",
    "            and np.any(times < old_timebase[0])\n",
    "            and not np.any(times == old_timebase[0])):\n",
    "        # Add an event at the beginning of the new timebase\n",
    "        interped_times = np.append(new_timebase[0], interped_times)\n",
    "        # If labels were provided, find the label of the first event before\n",
    "        # the old timebase and add it to the output labels\n",
    "        if labels is not None:\n",
    "            first_label = np.argmin(times < old_timebase[0]) - 1\n",
    "            valid_labels = [labels[first_label]] + valid_labels\n",
    "    # When labels were not provided, just return interpolated times\n",
    "    if labels is None:\n",
    "        return interped_times\n",
    "    # When labels were provided, return interpolated times and labels\n",
    "    else:\n",
    "        return interped_times, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extraction.AUDIO_HOP = 512\n",
    "errors = np.zeros(1000)\n",
    "def get_error():\n",
    "    fs = feature_extraction.AUDIO_FS\n",
    "    s = np.zeros((fs*np.random.randint(1, 5)))\n",
    "    place = np.random.randint(0, s.size)\n",
    "    s[place] = 1\n",
    "    gram = librosa.cqt(s, sr=fs, hop_length=feature_extraction.AUDIO_HOP,\n",
    "                       fmin=librosa.midi_to_hz(feature_extraction.NOTE_START),\n",
    "                       n_bins=feature_extraction.N_NOTES).T\n",
    "    return place/22050. - librosa.frames_to_time(\n",
    "        np.argmax(gram.sum(axis=1)),\n",
    "        hop_length=feature_extraction.AUDIO_HOP,\n",
    "        sr=feature_extraction.AUDIO_FS)[0]\n",
    "    #return place/22050. - feature_extraction.frame_times(gram)[np.argmax(gram.sum(axis=1))]\n",
    "errors = _ = joblib.Parallel(n_jobs=10, verbose=0)(\n",
    "    joblib.delayed(get_error)() for _ in range(1000))\n",
    "_ = plt.hist(errors, bins=20)\n",
    "print np.mean(errors), np.median(errors), np.max(errors) - np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('data', 'extracted')):\n",
    "    os.makedirs(os.path.join('data', 'extracted'))\n",
    "\n",
    "def process_one_file(diagnostics_file):\n",
    "    diagnostics = deepdish.io.load(diagnostics_file)\n",
    "    # Load the extracted features\n",
    "    midi_features = deepdish.io.load(diagnostics['midi_features_filename'])\n",
    "    audio_features = deepdish.io.load(\n",
    "        diagnostics['audio_features_filename'])\n",
    "    # Load in the original MIDI file\n",
    "    midi_object = pretty_midi.PrettyMIDI(str(diagnostics['midi_filename']))\n",
    "    # Compute the times of the frames (will be used for interpolation)\n",
    "    midi_frame_times = feature_extraction.frame_times(\n",
    "        midi_features['gram'])[diagnostics['aligned_midi_indices']]\n",
    "    audio_frame_times = feature_extraction.frame_times(\n",
    "        audio_features['gram'])[diagnostics['aligned_audio_indices']]\n",
    "    adjusted_beats = interpolate_times(\n",
    "        midi_object.get_beats(), midi_frame_times, audio_frame_times)\n",
    "    output_file = diagnostics_file.replace('diagnostics', 'extracted').replace('.h5', '.txt')\n",
    "    if not os.path.exists(os.path.split(output_file)[0]):\n",
    "        os.makedirs(os.path.split(output_file)[0])\n",
    "    np.savetxt(output_file, adjusted_beats)\n",
    "\n",
    "_ = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(process_one_file)(diagnostics_file)\n",
    "    for diagnostics_file in glob.glob(\n",
    "        os.path.join('data', 'diagnostics', '*', '*.h5*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate extracted ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(glob.glob('data/isophonics/*/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test a single example\n",
    "ref_beats, ref_labels = mir_eval.io.load_time_series('data/isophonics/01_-_Please_Please_Me/01_-_I_Saw_Her_Standing_There.txt')\n",
    "est_beats = mir_eval.io.load_events(\"data/extracted/01_-_Please_Please_Me/01_-_I_Saw_Her_Standing_There.txt0\")\n",
    "mir_eval.beat.evaluate(ref_beats, est_beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_reference_beat_variations(reference_beats):\n",
    "    # Create annotations at twice the metric level\n",
    "    interpolated_indices = np.arange(0, reference_beats.shape[0]-.5, .5)\n",
    "    original_indices = np.arange(0, reference_beats.shape[0])\n",
    "    double_reference_beats = np.interp(interpolated_indices,\n",
    "                                       original_indices,\n",
    "                                       reference_beats)\n",
    "    interpolated_indices = np.arange(0, reference_beats.shape[0]-.5, 1./3)\n",
    "    original_indices = np.arange(0, reference_beats.shape[0])\n",
    "    triple_reference_beats = np.interp(interpolated_indices,\n",
    "                                       original_indices,\n",
    "                                       reference_beats)\n",
    "    # Return metric variations:\n",
    "    # True, off-beat, double tempo, half tempo odd, and half tempo even\n",
    "    return (reference_beats,\n",
    "            double_reference_beats[1::2],\n",
    "            double_reference_beats,\n",
    "            reference_beats[::2],\n",
    "            reference_beats[1::2],\n",
    "            triple_reference_beats,\n",
    "            reference_beats[::3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(estimated_beats_file):\n",
    "    est_beats = mir_eval.io.load_events(estimated_beats_file)\n",
    "    if est_beats.size == 0:\n",
    "        return None, None\n",
    "    ground_truth_beats_file = os.path.splitext(estimated_beats_file.replace('extracted', 'isophonics'))[0] + '.txt'\n",
    "    ref_beats = mir_eval.io.load_labeled_events(ground_truth_beats_file)[0]\n",
    "    d = deepdish.io.load(estimated_beats_file.replace('extracted', 'diagnostics').replace('.txt', '.h5'))\n",
    "    confidence = d['score']\n",
    "    return confidence, mir_eval.beat.evaluate(ref_beats, est_beats)\n",
    "confidence_and_evaluation_scores = joblib.Parallel(n_jobs=10, verbose=10)(\n",
    "    joblib.delayed(get_scores)(estimated_beats_file)\n",
    "    for estimated_beats_file in glob.glob(os.path.join('data', 'extracted', '*', '*.txt*')))\n",
    "confidence_scores = [e[0] for e in confidence_and_evaluation_scores if e[0] is not None]\n",
    "evaluation_scores = [e[1] for e in confidence_and_evaluation_scores if e[1] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "#import mpld3\n",
    "#import mpld3.plugins\n",
    "#mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, sharex=\"col\", sharey=\"row\", figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    points = ax[n / 3, n % 3].scatter(\n",
    "        confidence_scores,\n",
    "        np.array([s[metric] for s in evaluation_scores]),\n",
    "        c='#3778bf',\n",
    "        alpha=.3)\n",
    "    dbn_mean = np.mean([s[metric] for s in dbn_scores])\n",
    "    ax[n / 3, n % 3].plot([-1, 2.], [dbn_mean, dbn_mean], 'k:', lw=4)\n",
    "    dbn_std = np.std([s[metric] for s in dbn_scores])\n",
    "    ax[n / 3, n % 3].set_xlim([-.05, 1.05])\n",
    "    ax[n / 3, n % 3].set_ylim([-.05, 1.05])\n",
    "    ax[n / 3, n % 3].set_title(metric)\n",
    "#mpld3.plugins.connect(fig, mpld3.plugins.LinkedBrush(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=\"col\", sharey=\"row\", figsize=(12, 3))\n",
    "for n, metric in enumerate(['F-measure', 'Any Metric Level Total', 'Information gain']):\n",
    "    points = ax[n].scatter(\n",
    "        confidence_scores,\n",
    "        np.array([s[metric] for s in evaluation_scores]),\n",
    "        c='#3778bf',\n",
    "        alpha=.3)\n",
    "    dbn_mean = np.mean([s[metric] for s in dbn_scores])\n",
    "    ax[n].plot([-1, 2.], [dbn_mean, dbn_mean], 'k:', lw=4)\n",
    "    dbn_std = np.std([s[metric] for s in dbn_scores])\n",
    "    ax[n].set_xlim([-.05, 1.05])\n",
    "    ax[n].set_ylim([-.05, 1.05])\n",
    "    ax[n].set_title(metric)\n",
    "    if n == 0:\n",
    "        ax[n].set_ylabel('Metric score')\n",
    "    if n == 1:\n",
    "        ax[n].set_xlabel('Confidence score')\n",
    "plt.savefig('beat_scores.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here](beat_scores.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to beat tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce beat annotations using [madmom](https://github.com/CPJKU/madmom)'s \"DBNBeatTracker\" (general-purpose algorithm, SOTA in 2014, not trained on the Beatles) run the following from the data folder:\n",
    "\n",
    "`mkdir dbn_annotations; DBNBeatTracker batch wav/*/*.wav -o dbn_annotations/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DBNBeatTracker flattens directory structure and names things \".beats.txt\", so let's fix that\n",
    "for f in glob.glob('data/isophonics/*/*.txt'):\n",
    "    new_filename = f.replace('isophonics', 'dbn_annotations')\n",
    "    old_filename = os.path.join(\n",
    "        'data', 'dbn_annotations', os.path.splitext(os.path.split(f)[1])[0] + '.beats.txt')\n",
    "    if not os.path.exists(os.path.split(new_filename)[0]):\n",
    "        os.makedirs(os.path.split(new_filename)[0])\n",
    "    shutil.move(old_filename, new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract beats with librosa\n",
    "for f in glob.glob('data/wav/*/*.wav'):\n",
    "    audio_data, _ = librosa.load(f)\n",
    "    beats = librosa.frames_to_time(librosa.beat.beat_track(audio_data)[1])\n",
    "    output_beats_file = f.replace('.wav', '.txt').replace('wav', 'librosa_annotations')\n",
    "    if not os.path.exists(os.path.split(output_beats_file)[0]):\n",
    "        os.makedirs(os.path.split(output_beats_file)[0])\n",
    "    np.savetxt(output_beats_file, beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_dir = 'dbn_annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbn_scores = []\n",
    "for estimated_beats_file in glob.glob(os.path.join('data', compare_dir, '*', '*.txt')):\n",
    "    ground_truth_beats_file = estimated_beats_file.replace(compare_dir, 'isophonics')\n",
    "    if not os.path.exists(ground_truth_beats_file):\n",
    "        continue\n",
    "    ref_beats, _ = mir_eval.io.load_labeled_events(ground_truth_beats_file)\n",
    "    est_beats = mir_eval.io.load_events(estimated_beats_file)\n",
    "    dbn_scores.append(mir_eval.beat.evaluate(ref_beats, est_beats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, .95, 95, endpoint=False)\n",
    "fig, ax = plt.subplots(3, 3, sharex=\"col\", figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    metric_scores = np.array([s[metric] for s in evaluation_scores])\n",
    "    mean_acc = np.array([np.median(metric_scores[confidence_scores > t]) for t in thresholds])\n",
    "    p25_acc = np.array([np.percentile(metric_scores[confidence_scores > t], 25) for t in thresholds])\n",
    "    p75_acc = np.array([np.percentile(metric_scores[confidence_scores > t], 75) for t in thresholds])\n",
    "    ax[n / 3, n % 3].plot(thresholds, mean_acc, c='#3778bf', lw=2)\n",
    "    ax[n / 3, n % 3].fill_between(thresholds, p25_acc, p75_acc, facecolor='#35ad6b', alpha=.2)\n",
    "    ax[n / 3, n % 3].set_title(metric)\n",
    "    dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    ax[n / 3, n % 3].plot([0, 1.], [dbn_median, dbn_median], 'k:', lw=4)\n",
    "    if metric == 'Information gain':\n",
    "        ax[n / 3, n % 3].set_ylim(0, .65)\n",
    "    else:\n",
    "        ax[n / 3, n % 3].set_ylim(.5, 1)\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.08, 'Confidence score', ha='center')\n",
    "fig.text(0.08, 0.5, 'Metric score', va='center', rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.violinplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacing = np.linspace(0, 1, 21)\n",
    "fig, ax = plt.subplots(3, 3, sharex=True, figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    scores = []\n",
    "    my_spacing = []\n",
    "    for start, end in zip(spacing[:-1], spacing[1:]):\n",
    "        scores_in_range = [s for m, s in zip(evaluation_scores, confidence_scores)\n",
    "                           if m[metric] >= start and m[metric] < end]\n",
    "        if len(scores_in_range) > 1:\n",
    "            scores.append(scores_in_range)\n",
    "            my_spacing.append(start)\n",
    "    plt.sca(ax[n / 3, n % 3])\n",
    "    plt.violinplot(scores, my_spacing, widths=np.diff(spacing)[0], showextrema=False)\n",
    "    #dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    #plt.plot([0, 1], [dbn_median, dbn_median], 'k:', lw=2)\n",
    "    #dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    #plt.plot([dbn_median, dbn_median], [0, 2*max(n)], 'k:', lw=2)\n",
    "    plt.title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for n, metric in enumerate([key for key in evaluation_scores[0].keys() if key != 'Goto']):\n",
    "    metric_scores = np.array([s[metric] for s in evaluation_scores])\n",
    "    plt.sca(ax[n / 3, n % 3])\n",
    "    n, _, _ = plt.hist(metric_scores, np.linspace(0, 1, 21), facecolor='#35ad6b', alpha=.5, orientation='horizontal')\n",
    "    dbn_median = np.median([s[metric] for s in dbn_scores])\n",
    "    plt.plot([0, 2*max(n)], [dbn_median, dbn_median], 'k:', lw=2)\n",
    "    plt.title(metric)\n",
    "    plt.xlim([0, max(n)*1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study particularly bad alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimated_beats_file = 'data/extracted/07_-_Revolver/02_-_Eleanor_Rigby.txt4'\n",
    "est_beats = mir_eval.io.load_events(estimated_beats_file)\n",
    "ground_truth_beats_file = os.path.splitext(estimated_beats_file.replace('extracted', 'isophonics'))[0] + '.txt'\n",
    "ref_beats = mir_eval.io.load_labeled_events(ground_truth_beats_file)[0]\n",
    "d = deepdish.io.load(estimated_beats_file.replace('extracted', 'diagnostics').replace('.txt', '.h5'))\n",
    "print mir_eval.beat.evaluate(ref_beats, est_beats)\n",
    "print d['score']\n",
    "\n",
    "#midi_object = pretty_midi.PrettyMIDI(str(d['output_midi_filename']))\n",
    "#m = midi_object.fluidsynth(22050)\n",
    "#a, fs = librosa.load(str(d['audio_filename']))\n",
    "#IPython.display.Audio([a, mir_eval.sonify.clicks(est_beats, 22050, length=a.shape[0])], rate=22050)\n",
    "plt.figure()\n",
    "plt.plot(np.diff(d['aligned_midi_indices']))\n",
    "plt.ylim([-.1, 1.1])\n",
    "plt.figure()\n",
    "plt.plot(np.diff(d['aligned_audio_indices']))\n",
    "plt.ylim([-.1, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d, metrics, ref_b, est_b in zip(diagnostics, evaluation_scores, ref_beats, est_beats):\n",
    "    if d['score'] > .6 and metrics['Information gain'] < .05:\n",
    "        audio_filename = d['audio_filename']\n",
    "        audio, fs = librosa.load(d['audio_filename'])\n",
    "        ref_clicks = mir_eval.sonify.clicks(\n",
    "            ref_b, fs, length=audio.shape[0])\n",
    "        est_clicks = mir_eval.sonify.clicks(\n",
    "            est_b, fs, length=audio.shape[0])\n",
    "        IPython.display.display(IPython.display.Audio([ref_clicks + audio, est_clicks + audio], rate=fs))\n",
    "        fig = plt.figure(figsize=(15, .5))\n",
    "        dist = np.min(np.abs(np.subtract.outer(est_b, ref_b)), axis=1)\n",
    "        plt.scatter(est_b, np.zeros(len(est_b)), c=dist, vmin=0, vmax=.25, alpha=.5, lw=0, cmap=plt.cm.jet)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import alignment_analysis\n",
    "midi_object = pretty_midi.PrettyMIDI(str(d['output_midi_filename']))\n",
    "IPython.display.Audio(alignment_analysis.synthesize_aligned_midi(audio, fs, midi_object), rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import djitw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio_gram = deepdish.io.load(str(d['audio_features_filename']))['gram']\n",
    "midi_gram = deepdish.io.load(str(d['midi_features_filenameusigram']\n",
    "D = 1 - np.dot(midi_gram, audio_gram.T)\n",
    "p, q, score = djitw.dtw(D, .96, np.median(D), inplace=False)\n",
    "mask = np.zeros_like(D)\n",
    "djitw.band_mask(.1, mask)\n",
    "print score/len(p)\n",
    "print score/len(p)/(np.sum(D*mask)/mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better similarity matrices for score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feature_extraction\n",
    "import scipy.spatial\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sims(diagnostics_file):\n",
    "    diagnostics = hickle.load(diagnostics_file)\n",
    "    \n",
    "    audio_data, fs = librosa.load(diagnostics['audio_filename'], sr=feature_extraction.AUDIO_FS)\n",
    "    audio_gram = librosa.cqt(\n",
    "        audio_data, sr=fs, hop_length=feature_extraction.AUDIO_HOP,\n",
    "        fmin=librosa.midi_to_hz(feature_extraction.NOTE_START), n_bins=feature_extraction.N_NOTES)\n",
    "    audio_gram = librosa.decompose.hpss(audio_gram)[0]\n",
    "    audio_gram = librosa.logamplitude(audio_gram, ref_power=audio_gram.max())\n",
    "    \n",
    "    midi_object = pretty_midi.PrettyMIDI(diagnostics['midi_filename'])\n",
    "    midi_audio = feature_extraction.fast_fluidsynth(midi_object, feature_extraction.MIDI_FS)\n",
    "    midi_gram = librosa.cqt(\n",
    "        midi_audio, sr=feature_extraction.MIDI_FS, hop_length=feature_extraction.MIDI_HOP,\n",
    "        fmin=librosa.midi_to_hz(feature_extraction.NOTE_START), n_bins=feature_extraction.N_NOTES)\n",
    "    midi_gram = librosa.decompose.hpss(midi_gram)[0]\n",
    "    midi_gram = librosa.logamplitude(midi_gram, ref_power=midi_gram.max())\n",
    "    \n",
    "    sim_cos = scipy.spatial.distance.cdist(midi_gram.T, audio_gram.T, 'cosine')\n",
    "    sim_euc = scipy.spatial.distance.cdist(midi_gram.T, audio_gram.T, 'euclidean')\n",
    "    \n",
    "    p, q = diagnostics['p'], diagnostics['q']\n",
    "    \n",
    "    return (sim_cos[p, q].mean()/sim_cos[p.min():p.max(), q.min():q.max()].mean(),\n",
    "            sim_euc[p, q].mean()/sim_euc[p.min():p.max(), q.min():q.max()].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = joblib.Parallel(n_jobs=10, verbose=51)(\n",
    "    joblib.delayed(get_sims)(d) for d in results['diagnostics_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpld3.plugins.PointHTMLTooltip??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mpld3\n",
    "import mpld3.plugins\n",
    "mpld3.enable_notebook()\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointHTMLTooltipStay(mpld3.plugins.PluginBase):\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"htmltooltip\", HtmlTooltipPlugin);\n",
    "    HtmlTooltipPlugin.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    HtmlTooltipPlugin.prototype.constructor = HtmlTooltipPlugin;\n",
    "    HtmlTooltipPlugin.prototype.requiredProps = [\"id\"];\n",
    "    HtmlTooltipPlugin.prototype.defaultProps = {labels:null, hoffset:0, voffset:10};\n",
    "    function HtmlTooltipPlugin(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    HtmlTooltipPlugin.prototype.draw = function(){\n",
    "       var obj = mpld3.get_element(this.props.id);\n",
    "       var labels = this.props.labels;\n",
    "       var tooltip = d3.select(\"body\").append(\"div\")\n",
    "                    .attr(\"class\", \"mpld3-tooltip\")\n",
    "                    .style(\"position\", \"absolute\")\n",
    "                    .style(\"z-index\", \"10\")\n",
    "                    .style(\"visibility\", \"hidden\");\n",
    "\n",
    "       obj.elements()\n",
    "           .on(\"mouseover\", function(d, i){\n",
    "                              tooltip.html(labels[i])\n",
    "                                     .style(\"visibility\", \"visible\");})\n",
    "           .on(\"mousemove\", function(d, i){\n",
    "                    tooltip\n",
    "                      .style(\"top\", d3.event.pageY + this.props.voffset + \"px\")\n",
    "                      .style(\"left\",d3.event.pageX + this.props.hoffset + \"px\");\n",
    "                 }.bind(this))\n",
    "           .on(\"mousedown\",  function(d, i){\n",
    "                           tooltip.style(\"visibility\", \"hidden\");});\n",
    "    };\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points, labels=None,\n",
    "                 hoffset=0, voffset=10, css=None):\n",
    "        self.points = points\n",
    "        self.labels = labels\n",
    "        self.voffset = voffset\n",
    "        self.hoffset = hoffset\n",
    "        self.css_ = css or \"\"\n",
    "        if isinstance(points, matplotlib.lines.Line2D):\n",
    "            suffix = \"pts\"\n",
    "        else:\n",
    "            suffix = None\n",
    "        self.dict_ = {\"type\": \"htmltooltip\",\n",
    "                      \"id\": mpld3.plugins.get_id(points, suffix),\n",
    "                      \"labels\": labels,\n",
    "                      \"hoffset\": hoffset,\n",
    "                      \"voffset\": voffset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "points = ax.plot(results['F-measure'],\n",
    "                 np.array(results['score']) - np.array([s[0] for s in sims]),\n",
    "                 'o', color='b', mec='k', ms=15, mew=1, alpha=.6)\n",
    "\n",
    "labels = ['<input type=\"text\" size={} value=\"{}\" />'.format(len(d), d) for d in results['diagnostics_filename']]\n",
    "tooltip = PointHTMLTooltipStay(\n",
    "    points[0], labels, voffset=10, hoffset=10)\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    print k, len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "bad_ones = []\n",
    "for f, a, i, s, d, fn in zip(results['F-measure'], results['Any Metric Level Total'],\n",
    "                             results['Information gain'], results['score'],\n",
    "                             np.array(results['score']) - np.array([s[0] for s in sims]),\n",
    "                             results['diagnostics_filename']):\n",
    "    if d > .1:\n",
    "        bad_ones.append([f, a, i, d, os.path.split(fn)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tabulate.tabulate(bad_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diagnostics_file = 'data/diagnostics/10 - The Beatles Disc 1 - 08 - Happiness is a Warm Gun.h52'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Key detection experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "import glob\n",
    "import pretty_midi\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import librosa\n",
    "import vamp\n",
    "import deepdish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key loading/computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_midi_key(filename):\n",
    "    ''' Load in key labels from a MIDI file '''\n",
    "    # Load in MIDI object and grab key change events\n",
    "    pm = pretty_midi.PrettyMIDI(filename)\n",
    "    key_changes = pm.key_signature_changes\n",
    "    # Convert each key change's number to a string (like 'C Major')\n",
    "    # Also convert it to lowercase, for mir_eval's sake\n",
    "    return [pretty_midi.key_number_to_key_name(k.key_number).lower()\n",
    "            for k in key_changes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_isophonics_key(filename):\n",
    "    ''' Read in key labels from an isophonics lab file '''\n",
    "    # Isophonics key lab files have three columns:\n",
    "    # start time, end time, and label\n",
    "    start, end, labels = mir_eval.io.load_delimited(\n",
    "        filename, [float, float, str])\n",
    "    # Extract key labels, which in lab files are formatted as\n",
    "    # 'key\\tC' or 'key\\tC:minor'\n",
    "    keys = [l.split('\\t')[1] for l in labels if 'Key' in l]\n",
    "    # Convert from 'C' and 'C:minor' to 'c major' and 'c minor'\n",
    "    for n, key in enumerate(keys):\n",
    "        if 'minor' in key:\n",
    "            keys[n] = key.replace(':', ' ').lower()\n",
    "        else:\n",
    "            keys[n] = key.lower() + ' major'\n",
    "        # Validate the key early\n",
    "        mir_eval.key.validate_key(keys[n])\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_vamp_key(filename):\n",
    "    ''' Estimate the key from an audio file using QM key detector '''\n",
    "    # Load in audio data at its native sampling rate\n",
    "    audio_data, fs = librosa.load(filename, sr=None)\n",
    "    # Create a vamp processor that will generate key labels\n",
    "    key_generator = vamp.process_audio_multiple_outputs(\n",
    "        audio_data, fs, 'qm-vamp-plugins:qm-keydetector', ['key'])\n",
    "    # Grab the key labels produced by the vampplugin\n",
    "    vamp_output = [out['key'] for out in key_generator]\n",
    "    keys = [l['label'] for l in vamp_output]\n",
    "    # Compute the durations of each key in the song\n",
    "    starts = [float(l['timestamp']) for l in vamp_output]\n",
    "    starts.append(librosa.get_duration(audio_data, fs))\n",
    "    durations = np.diff(starts)\n",
    "    unique_keys = list(set(keys))\n",
    "    key_durations = [sum(d for k, d in zip(keys, durations) if k == key)\n",
    "                     for key in unique_keys]\n",
    "    # Retrieve the key which spanned the most of the song\n",
    "    most_common_key = unique_keys[np.argmax(key_durations)]\n",
    "    # Sometimes vamp produces keys like\n",
    "    # 'Eb / D# minor'\n",
    "    # so here, we are just retrieving the last part ('D# minor')\n",
    "    if ' / ' in most_common_key:\n",
    "        most_common_key = most_common_key.split(' / ')[1]\n",
    "    return most_common_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of different key estimations/annotations compared to isophonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file for each source\n",
    "scores = collections.defaultdict(list)\n",
    "# Keep track of whether each MIDI estimated key is C major\n",
    "c_majors = []\n",
    "for lab_filename in glob.glob('data/isophonics_key/*/*.lab'):\n",
    "    # Load Isophonics key from .lab file\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(lab_filename)\n",
    "    except Exception as e:\n",
    "        # Keep track of how many isophonics files which have invalid keys\n",
    "        print 'Error for {}: {}'.format(lab_filename, e)\n",
    "        n_skipped['isophonics_invalid'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['>1_isophonics_keys'] += 1\n",
    "        continue\n",
    "    isophonics_key = isophonics_keys[0]\n",
    "    \n",
    "    # Loop over all possible MIDI files for this key\n",
    "    midi_glob = lab_filename.replace('isophonics_key', 'mid').replace('.lab', '.mid*')\n",
    "    for midi_filename in glob.glob(midi_glob):\n",
    "        # Get keys from MIDI file\n",
    "        try:\n",
    "            midi_keys = load_midi_key(midi_filename)\n",
    "        except Exception as e:\n",
    "            print 'Error for {}: {}'.format(midi_filename, e)\n",
    "            n_skipped['midi_exceptions'] += 1\n",
    "            continue\n",
    "        # If there's no key change event, skip\n",
    "        if len(midi_keys) == 0:\n",
    "            n_skipped['no_midi_keys'] += 1\n",
    "            continue\n",
    "        # If there's multiple key change events, skip\n",
    "        if len(midi_keys) > 1:\n",
    "            n_skipped['>1_midi_keys'] += len(midi_keys) > 1\n",
    "            continue\n",
    "        midi_key = midi_keys[0]\n",
    "        # Keep track of whether the estimated key was a C major\n",
    "        c_majors.append(midi_keys[0] == 'c major')\n",
    "        # Compute and store score for this MIDI file\n",
    "        scores['midi'].append(mir_eval.key.weighted_score(isophonics_key, midi_key))\n",
    "\n",
    "    # Construct .wav filename from .lab filename\n",
    "    audio_filename = lab_filename.replace('isophonics_key', 'wav').replace('.lab', '.wav')\n",
    "    # Estimate the key using vamp QM key detector plugin\n",
    "    try:\n",
    "        vamp_key = load_vamp_key(audio_filename)\n",
    "    except Exception as e:\n",
    "        print 'Error for {}: {}'.format(audio_filename, e)\n",
    "        n_skipped['audio_exceptions'] += 1\n",
    "        continue\n",
    "    scores['vamp'].append(mir_eval.key.weighted_score(isophonics_key, vamp_key))\n",
    "\n",
    "    # Construct whatkeyisitin text filename from .lab filename\n",
    "    whatkeyisitin_filename = lab_filename.replace('isophonics_key', 'whatkeyisitin_key').replace('.lab', '.txt')\n",
    "    if not os.path.exists(whatkeyisitin_filename):\n",
    "        # Keep track of how many are skipped due to missing wkiii annotation\n",
    "        n_skipped['no_wkiii_file'] += 1\n",
    "        continue\n",
    "    with open(whatkeyisitin_filename) as f:\n",
    "        whatkeyisitin_key = f.read()\n",
    "    scores['wkiii'].append(mir_eval.key.weighted_score(isophonics_key, whatkeyisitin_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print statistics about the MIDI key accuracy\n",
    "for key, value in n_skipped.items():\n",
    "    print '{} skipped because {}'.format(value, key)\n",
    "print 'Total isophonics .lab files: {}'.format(len(glob.glob('data/isophonics_key/*/*.lab')))\n",
    "print\n",
    "mean_scores = collections.OrderedDict([\n",
    "    ('MIDI, all keys', np.mean(scores['midi'])),\n",
    "    ('MIDI, C major only', np.mean([s for c, s in zip(c_majors, scores['midi']) if c])),\n",
    "    ('MIDI, non-C major', np.mean([s for c, s in zip(c_majors, scores['midi']) if not c])),\n",
    "    ('QM Key Detector', np.mean(scores['vamp'])),\n",
    "    ('whatkeyisitin.com', np.mean(scores['wkiii']))])\n",
    "n_comparisons = collections.OrderedDict([\n",
    "    ('MIDI, all keys', len(scores['midi'])),\n",
    "    ('MIDI, C major only', sum(c_majors)),\n",
    "    ('MIDI, non-C major', len([c for c in c_majors if not c])),\n",
    "    ('QM Key Detector', len(scores['vamp'])),\n",
    "    ('whatkeyisitin.com', len(scores['wkiii']))])\n",
    "print tabulate.tabulate(\n",
    "    [(name, score, num) for (name, score, num) in \n",
    "     zip(mean_scores.keys(), mean_scores.values(), n_comparisons.values())],\n",
    "    ['Source', 'Mean score', '# of comparisons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(len(s['key_numbers']) for s in statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([k for k in sum([s['key_numbers'] for s in statistics], []) if k == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "ts = np.linspace(0, 1, 101)\n",
    "a = [np.mean([s for c, s, a in zip(c_majors, scores, alignment_scores) if not c and a > t]) for t in ts]\n",
    "plt.plot(ts, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of vamp keys compared to isophonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file\n",
    "scores = []\n",
    "for audio_filename in glob.glob('data/wav/*/*.wav'):\n",
    "\n",
    "    \n",
    "    # Construct .lab file path from .wav file path\n",
    "    base_path, filename = os.path.split(audio_filename)\n",
    "    lab_filename = os.path.join(base_path.replace('wav', 'isophonics_key'),\n",
    "                                os.path.splitext(filename)[0] + '.lab')\n",
    "    if not os.path.exists(lab_filename):\n",
    "        n_skipped['no_lab_file'] += 1\n",
    "        continue\n",
    "    # Load in Isophonics keys from .lab file\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(lab_filename)\n",
    "    except Exception as e:\n",
    "        print 'Error for {}: {}'.format(lab_filename, e)\n",
    "        n_skipped['isophonics_exceptions'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['>1_isophonics_keys'] += 1\n",
    "        continue\n",
    "    # Compute and store score for this wav file\n",
    "    scores.append(mir_eval.key.weighted_score(isophonics_keys[0], vamp_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total possible: {}'.format(len(glob.glob('data/wav/*/*.wav')))\n",
    "print '# of audio exceptions: {}'.format(n_skipped['audio_exceptions'])\n",
    "print '# of Isophonics exceptions: {}'.format(n_skipped['isophonics_exceptions'])\n",
    "print '# of Isophonics with >1 keys: {}'.format(n_skipped['>1_isophonics_keys'])\n",
    "print '# of valid comparisons: {}'.format(len(scores))\n",
    "print 'Accuracy all: {:.3f}'.format(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/whatkeyisitin.txt') as f:\n",
    "    whatkeyisitin_list = [l.strip().split('\\t') for l in f]\n",
    "# Load in beatles index for searching\n",
    "beatles_index = whoosh_search.get_whoosh_index(\n",
    "    os.path.join('data', 'index'))\n",
    "with beatles_index.searcher() as searcher:\n",
    "    # Get list of beatles entries; we will use this to retrieve paths.\n",
    "    beatles_list = list(searcher.documents())\n",
    "    for (title, key) in whatkeyisitin_list:\n",
    "        # Search the beatles index for this track\n",
    "        matches = whoosh_search.search(searcher, beatles_index.schema, 'The Beatles', fix(title), 9)\n",
    "        # If we have this beatles track in the index\n",
    "        if len(matches) > 0:\n",
    "            # Grab the base path for this track\n",
    "            path = [e['path'] for e in beatles_list if e['id'] == matches[0][0]][0]\n",
    "            output_filename = os.path.join('data', 'whatkeyisitin_key', path + '.txt')\n",
    "            if not os.path.exists(os.path.split(output_filename)[0]):\n",
    "                os.makedirs(os.path.split(output_filename)[0])\n",
    "            with open(output_filename, 'wb') as f:\n",
    "                f.write(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of the number of files skipped for different reasons\n",
    "n_skipped = collections.defaultdict(int)\n",
    "# Keep track of the weighted accuracy for each file\n",
    "scores = []\n",
    "for whatkeyisitin_file in glob.glob('data/whatkeyisitin_key/*/*.txt'):\n",
    "    with open(whatkeyisitin_file) as f:\n",
    "        whatkeyisitin_key = f.read()\n",
    "    isophonics_file = whatkeyisitin_file.replace('whatkeyisitin_key', 'isophonics_key').replace('.txt', '.lab')\n",
    "    try:\n",
    "        isophonics_keys = load_isophonics_key(isophonics_file)\n",
    "    except Exception as e:\n",
    "        print 'Error for {}: {}'.format(lab_filename, e)\n",
    "        n_skipped['isophonics_exceptions'] += 1\n",
    "        continue\n",
    "    # If there are more than 1 Isophonics keys, skip\n",
    "    if len(isophonics_keys) > 1:\n",
    "        n_skipped['>1_isophonics_keys'] += 1\n",
    "        continue\n",
    "    scores.append(mir_eval.key.weighted_score(isophonics_keys[0], whatkeyisitin_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total possible: {}'.format(len(glob.glob('data/whatkeyisitin_key/*/*.txt')))\n",
    "print '# of Isophonics exceptions: {}'.format(n_skipped['isophonics_exceptions'])\n",
    "print '# of Isophonics with >1 keys: {}'.format(n_skipped['>1_isophonics_keys'])\n",
    "print '# of valid comparisons: {}'.format(len(scores))\n",
    "print 'Accuracy all: {:.3f}'.format(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import midi\n",
    "import glob\n",
    "import os\n",
    "import joblib\n",
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_names(midi_file):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def any_lyrics(midi_file):\n",
    "    try:\n",
    "        midi_data = midi.read_midifile(midi_file)\n",
    "    except:\n",
    "        return None\n",
    "    for track in midi_data:\n",
    "        for event in track:\n",
    "            if isinstance(event, midi.LyricsEvent):\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "has_lyrics = joblib.Parallel(n_jobs=10, verbose=11)(\n",
    "    joblib.delayed(any_lyrics)(f) for f in\n",
    "    glob.glob(os.path.join('data', 'unique_mid', '*', '*.mid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sum([l for l in has_lyrics if l is not None])\n",
    "print sum([l for l in has_lyrics if l is not None])/float(len(has_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_bad = 0\n",
    "for f in glob.glob('data/mid/*/*.mid*'):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(f)\n",
    "    except:\n",
    "        n_bad += 1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_mids = []\n",
    "beatles_wavs = glob.glob('data/wav/*/*.wav')\n",
    "for f in beatles_wavs:\n",
    "    mid_glob = f.replace('wav', 'mid') + '*'\n",
    "    n_mids.append(len(glob.glob(mid_glob)))\n",
    "n_mids = np.array(n_mids)\n",
    "print len(n_mids)\n",
    "print np.sum(n_mids == 0)\n",
    "print np.median(n_mids)\n",
    "print np.max(n_mids)\n",
    "print beatles_wavs[np.argmax(n_mids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Time signature extra data values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_time_signature_extra_data(midi_file):\n",
    "    try:\n",
    "        midi_data = midi.read_midifile(midi_file)\n",
    "    except:\n",
    "        return None\n",
    "    metronome = []\n",
    "    thirtyseconds = []\n",
    "    for track in midi_data:\n",
    "        for event in track:\n",
    "            if isinstance(event, midi.TimeSignatureEvent):\n",
    "                metronome.append(event.metronome)\n",
    "                thirtyseconds.append(event.thirtyseconds)\n",
    "    return metronome, thirtyseconds\n",
    "\n",
    "time_signature_data = joblib.Parallel(n_jobs=10, verbose=11)(\n",
    "    joblib.delayed(get_time_signature_extra_data)(f) for f in\n",
    "    glob.glob(os.path.join('data', 'unique_mid', '*', '*.mid'))[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metronome = sum([t[0] for t in time_signature_data if t is not None], [])\n",
    "thirtyseconds = sum([t[1] for t in time_signature_data if t is not None], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "n = float(len(metronome))\n",
    "for k, v in sorted(collections.Counter(metronome).items(), key=lambda x: -x[1]):\n",
    "    print '{}: {:.2f}%'.format(k, 100*v/n)\n",
    "    if 100*v/n < 1:\n",
    "        break\n",
    "print\n",
    "for k, v in sorted(collections.Counter(thirtyseconds).items(), key=lambda x: -x[1]):\n",
    "    print '{}: {:.2f}%'.format(k, 100*v/n)\n",
    "    if 100*v/n < .1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_time_signature_extra_data(midi_file):\n",
    "    try:\n",
    "        midi_data = midi.read_midifile(midi_file)\n",
    "    except:\n",
    "        return None\n",
    "    for track in midi_data:\n",
    "        for event in track:\n",
    "            if isinstance(event, midi.TimeSignatureEvent):\n",
    "                if event.thirtyseconds != 8:\n",
    "                    print event.thirtyseconds, midi_file\n",
    "\n",
    "time_signature_data = joblib.Parallel(n_jobs=10, verbose=11)(\n",
    "    joblib.delayed(get_time_signature_extra_data)(f) for f in\n",
    "    glob.glob(os.path.join('data', 'unique_mid', '*', '*.mid'))[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm = pretty_midi.PrettyMIDI('data/unique_mid/f/f9e6f020ba4ae161d5d6b083668b3eeb.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for track in midi.read_midifile('data/unique_mid/f/f9e6f020ba4ae161d5d6b083668b3eeb.mid'):\n",
    "    for event in track:\n",
    "        if isinstance(event, midi.TimeSignatureEvent):\n",
    "            print event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(pm.fluidsynth(22050), rate=22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
